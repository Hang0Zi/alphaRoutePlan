import sys
import os
import pickle
import gzip
from datetime import datetime
import numpy as np
import time as time_module
from collections import defaultdict
from typing import Dict, Tuple
from pathlib import Path  # â† æ·»åŠ è¿™è¡Œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å…¨å±€å˜é‡ï¼šæ•°æ®ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# åŸæœ‰å…¨å±€æ•°æ®å˜é‡
G_GLOBAL = None
SPARSE_DATA_GLOBAL = None
NODE_TO_INDEX_GLOBAL = None
SCENARIO_DATES_GLOBAL = None
SCENARIO_PROBS_GLOBAL = None
TIME_INTERVALS_PER_DAY_GLOBAL = None

# âœ¨ æ–°å¢ï¼šé¢„è®¡ç®—çš„æ•°æ®ç»“æ„
ADJ_LIST_FORWARD_GLOBAL = None  # æ­£å‘é‚»æ¥è¡¨
ADJ_LIST_BACKWARD_GLOBAL = None  # åå‘é‚»æ¥è¡¨
LINK_DISTRIBUTIONS_GLOBAL = None  # é“¾è·¯åˆ†å¸ƒ
LINK_DISTRIBUTIONS_BACKWARD_GLOBAL = None

DATA_LOADED = False

def load_data_once(data_path=None, cache_file='precomputed_data.pkl.gz', force_rebuild=False):
    """
    å…¨å±€åŠ è½½æ•°æ®ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    âœ¨ æ–°å¢ï¼šåŒæ—¶é¢„è®¡ç®—é‚»æ¥è¡¨å’Œé“¾è·¯åˆ†å¸ƒï¼Œæ”¯æŒç¼“å­˜
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼‰
    """
    global G_GLOBAL, SPARSE_DATA_GLOBAL, NODE_TO_INDEX_GLOBAL, INDEX_TO_NODE_GLOBAL
    global SCENARIO_DATES_GLOBAL, SCENARIO_PROBS_GLOBAL, TIME_INTERVALS_PER_DAY_GLOBAL
    global ADJ_LIST_FORWARD_GLOBAL, ADJ_LIST_BACKWARD_GLOBAL, LINK_DISTRIBUTIONS_GLOBAL
    global LINK_DISTRIBUTIONS_BACKWARD_GLOBAL  # â† æ–°å¢ï¼šç”¨äºåå‘æ±‚è§£
    global DATA_LOADED
    
    if DATA_LOADED and not force_rebuild:
        print("æ•°æ®å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½")
        return (G_GLOBAL, SPARSE_DATA_GLOBAL, NODE_TO_INDEX_GLOBAL,
                SCENARIO_DATES_GLOBAL, SCENARIO_PROBS_GLOBAL, TIME_INTERVALS_PER_DAY_GLOBAL)
    
    # å¯¼å…¥config
    try:
        import config as config
        if data_path is None: 
            data_path = config.DATA_PATH
    except: 
        if data_path is None:
            data_path = 'data/test_data.pkl.gz'
    
    print(f"\n{'='*70}")
    print(f"åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆ - æ”¯æŒç¼“å­˜ï¼‰")
    print(f"{'='*70}")
    print(f"  æ•°æ®æ–‡ä»¶: {data_path}")
    print(f"  ç¼“å­˜æ–‡ä»¶: {cache_file}")
    
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    start_time = time_module.time()
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # æ­¥éª¤1ï¼šåŠ è½½åŸºç¡€æ•°æ®
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    print(f"\n[1/4] åŠ è½½åŸºç¡€æ•°æ®...")
    with gzip.open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    G_GLOBAL = data['G']
    SPARSE_DATA_GLOBAL = data['sparse_data']
    NODE_TO_INDEX_GLOBAL = data['node_to_index']
    INDEX_TO_NODE_GLOBAL = {v: k for k, v in NODE_TO_INDEX_GLOBAL.items()}
    SCENARIO_DATES_GLOBAL = [datetime.strptime(d, '%Y-%m-%d').date() 
                              for d in data['scenario_dates']]
    SCENARIO_PROBS_GLOBAL = data['scenario_probs']
    TIME_INTERVALS_PER_DAY_GLOBAL = data['time_intervals_per_day']
    
    print(f"  âœ“ åŸºç¡€æ•°æ®åŠ è½½å®Œæˆ")
    print(f"    èŠ‚ç‚¹æ•°: {len(G_GLOBAL.nodes()):,}")
    print(f"    è¾¹æ•°: {len(G_GLOBAL.edges()):,}")
    print(f"    åœºæ™¯æ•°: {len(SCENARIO_DATES_GLOBAL)}")
    print(f"    æ—¶é—´ç‰‡æ•°/å¤©: {TIME_INTERVALS_PER_DAY_GLOBAL}")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # æ­¥éª¤2ï¼šå°è¯•ä»ç¼“å­˜åŠ è½½é¢„è®¡ç®—æ•°æ®
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    cache_loaded = False
    
    if not force_rebuild and Path(cache_file).exists():
        print(f"\n[2/4] å°è¯•ä»ç¼“å­˜åŠ è½½é¢„è®¡ç®—æ•°æ®...")
        cache_result = load_precomputed_data(cache_file)
        
        if cache_result is not None:
            ADJ_LIST_FORWARD_GLOBAL, ADJ_LIST_BACKWARD_GLOBAL, LINK_DISTRIBUTIONS_GLOBAL, LINK_DISTRIBUTIONS_BACKWARD_GLOBAL = cache_result
            cache_loaded = True
            print(f"  ğŸš€ ä»ç¼“å­˜åŠ è½½æˆåŠŸï¼")
        else:
            print(f"  âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°è®¡ç®—")
    else:
        if force_rebuild:
            print(f"\n[2/4] å¼ºåˆ¶é‡å»ºé¢„è®¡ç®—æ•°æ®...")
        else:
            print(f"\n[2/4] ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦è®¡ç®—é¢„è®¡ç®—æ•°æ®...")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # æ­¥éª¤3ï¼šå¦‚æœç¼“å­˜æœªåŠ è½½ï¼Œé‡æ–°è®¡ç®—å¹¶ä¿å­˜
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if not cache_loaded:
        print(f"\n[3/4] è®¡ç®—é¢„è®¡ç®—æ•°æ®...")
        
        # 3.1 æ„å»ºé‚»æ¥è¡¨
        print(f"  [3.1] æ„å»ºé‚»æ¥è¡¨...")
        adj_start = time_module.time()
        
        ADJ_LIST_FORWARD_GLOBAL, ADJ_LIST_BACKWARD_GLOBAL = _build_adjacency_lists(
            SPARSE_DATA_GLOBAL,
            NODE_TO_INDEX_GLOBAL,
            len(SCENARIO_DATES_GLOBAL)
        )
        
        adj_time = time_module.time() - adj_start
        print(f"    âœ“ é‚»æ¥è¡¨æ„å»ºå®Œæˆ (ç”¨æ—¶ {adj_time:.2f}ç§’)")
        print(f"      æ­£å‘é‚»æ¥è¡¨: {len(ADJ_LIST_FORWARD_GLOBAL)} ä¸ªèŠ‚ç‚¹")
        print(f"      åå‘é‚»æ¥è¡¨:  {len(ADJ_LIST_BACKWARD_GLOBAL)} ä¸ªèŠ‚ç‚¹")
        
        # 3.2 é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒï¼ˆåŒæ—¶ç”Ÿæˆæ­£å‘å’Œåå‘ï¼‰
        print(f"  [3.2] é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ...")
        dist_start = time_module.time()
        
        # ä¿®æ”¹ï¼šæ¥æ”¶ä¸¤ä¸ªè¿”å›å€¼
        LINK_DISTRIBUTIONS_GLOBAL, LINK_DISTRIBUTIONS_BACKWARD_GLOBAL = _precompute_link_distributions(
            SPARSE_DATA_GLOBAL,
            NODE_TO_INDEX_GLOBAL,
            len(SCENARIO_DATES_GLOBAL)
        )
        
        dist_time = time_module.time() - dist_start
        print(f"    âœ“ é“¾è·¯åˆ†å¸ƒè®¡ç®—å®Œæˆ (ç”¨æ—¶ {dist_time:.2f}ç§’)")
        
        # 3.3 ä¿å­˜åˆ°ç¼“å­˜
        print(f"\n  [3.3] ä¿å­˜é¢„è®¡ç®—æ•°æ®åˆ°ç¼“å­˜...")
        save_precomputed_data(
            ADJ_LIST_FORWARD_GLOBAL,
            ADJ_LIST_BACKWARD_GLOBAL,
            LINK_DISTRIBUTIONS_GLOBAL,
            LINK_DISTRIBUTIONS_BACKWARD_GLOBAL,  # â† æ–°å¢å‚æ•°
            filename=cache_file
        )
    else:
        # å¦‚æœä»ç¼“å­˜åŠ è½½ï¼Œè·³è¿‡æ­¥éª¤3
        print(f"\n[3/4] âœ“ è·³è¿‡è®¡ç®—ï¼ˆå·²ä»ç¼“å­˜åŠ è½½ï¼‰")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # æ­¥éª¤4ï¼šå®Œæˆ
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    load_time = time_module.time() - start_time
    
    print(f"\n[4/4] âœ“ æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å®Œæˆï¼")
    print(f"  æ€»è€—æ—¶: {load_time:.2f}ç§’")
    if cache_loaded:
        print(f"    åŸºç¡€æ•°æ®åŠ è½½: {load_time:.2f}ç§’")
        print(f"    é¢„è®¡ç®—æ•°æ®:  ä»ç¼“å­˜åŠ è½½ ğŸš€")
    else:
        print(f"    åŸºç¡€æ•°æ®åŠ è½½: çº¦ {(load_time - adj_time - dist_time):.2f}ç§’")
        print(f"    é‚»æ¥è¡¨æ„å»º: {adj_time:.2f}ç§’")
        print(f"    é“¾è·¯åˆ†å¸ƒè®¡ç®—: {dist_time:.2f}ç§’")
        print(f"    ç¼“å­˜ä¿å­˜: å·²å®Œæˆ ğŸ’¾")
    print(f"{'='*70}\n")
    
    DATA_LOADED = True
    
    return (G_GLOBAL, SPARSE_DATA_GLOBAL, NODE_TO_INDEX_GLOBAL,
            SCENARIO_DATES_GLOBAL, SCENARIO_PROBS_GLOBAL, TIME_INTERVALS_PER_DAY_GLOBAL)


def save_precomputed_data(adj_list_forward, adj_list_backward, 
                          link_distributions_forward, link_distributions_backward,
                          filename='precomputed_data.pkl.gz'):
    """ä¿å­˜é¢„è®¡ç®—æ•°æ®ï¼ˆå‹ç¼©ï¼ŒåŒ…å«æ­£å‘å’Œåå‘é“¾è·¯åˆ†å¸ƒï¼‰"""
    print(f"\n{'='*70}")
    print(f"ä¿å­˜é¢„è®¡ç®—æ•°æ®")
    print(f"{'='*70}")
    
    data = {
        'adj_list_forward': dict(adj_list_forward),
        'adj_list_backward':  dict(adj_list_backward),
        'link_distributions_forward': link_distributions_forward,  # â† ä¿®æ”¹
        'link_distributions_backward':  link_distributions_backward,  # â† æ–°å¢
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'forward_nodes': len(adj_list_forward),
            'reverse_nodes': len(adj_list_backward),
            'forward_edges': sum(len(v) for v in adj_list_forward.values()),
            'reverse_edges': sum(len(v) for v in adj_list_backward.values()),
            'distributions_forward': len(link_distributions_forward),  # â† ä¿®æ”¹
            'distributions_backward':  len(link_distributions_backward)  # â† æ–°å¢
        }
    }
    
    start_time = time_module.time()
    
    # ä¿å­˜ä¸ºå‹ç¼©æ–‡ä»¶
    with gzip.open(filename, 'wb', compresslevel=6) as f:
        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    file_size = os.path.getsize(filename)
    elapsed = time_module.time() - start_time
    
    print(f"  âœ“ å·²ä¿å­˜:  {filename}")
    print(f"  æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
    print(f"  è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"{'='*70}\n")

def load_precomputed_data(filename='precomputed_data.pkl.gz'):
    """åŠ è½½é¢„è®¡ç®—æ•°æ®ï¼ˆå‹ç¼©ï¼ŒåŒ…å«æ­£å‘å’Œåå‘é“¾è·¯åˆ†å¸ƒï¼‰"""
    if not os.path.exists(filename):
        return None
    
    print(f"\n{'='*70}")
    print(f"åŠ è½½é¢„è®¡ç®—æ•°æ®")
    print(f"{'='*70}")
    
    start_time = time_module.time()
    
    try:
        with gzip.open(filename, 'rb') as f:
            data = pickle.load(f)
        
        elapsed = time_module.time() - start_time
        
        metadata = data.get('metadata', {})
        
        print(f"  âœ“ å·²åŠ è½½: {filename}")
        print(f"  æ•°æ®æ—¶é—´:  {metadata.get('timestamp', 'unknown')}")
        print(f"  æ­£å‘èŠ‚ç‚¹:  {metadata.get('forward_nodes', 0):,}")
        print(f"  åå‘èŠ‚ç‚¹: {metadata.get('reverse_nodes', 0):,}")
        print(f"  æ­£å‘è¾¹æ•°: {metadata.get('forward_edges', 0):,}")
        print(f"  åå‘è¾¹æ•°: {metadata.get('reverse_edges', 0):,}")
        print(f"  æ­£å‘é“¾è·¯åˆ†å¸ƒ: {metadata.get('distributions_forward', 0):,}")
        print(f"  åå‘é“¾è·¯åˆ†å¸ƒ: {metadata.get('distributions_backward', 0):,}")
        print(f"  åŠ è½½è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"{'='*70}\n")
        
        # è½¬æ¢å› defaultdict
        from collections import defaultdict
        adj_forward = defaultdict(list, data['adj_list_forward'])
        adj_backward = defaultdict(list, data['adj_list_backward'])
        
        # å…¼å®¹æ—§ç‰ˆæœ¬ç¼“å­˜ï¼ˆåªæœ‰ä¸€ä¸ªlink_distributionsï¼‰
        if 'link_distributions_forward' in data:
            link_dists_forward = data['link_distributions_forward']
            link_dists_backward = data.get('link_distributions_backward', link_dists_forward)
        else:
            # æ—§ç‰ˆæœ¬ç¼“å­˜
            print(f"  âš ï¸ æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬ç¼“å­˜ï¼Œæ­£å‘å’Œåå‘ä½¿ç”¨ç›¸åŒåˆ†å¸ƒ")
            link_dists_forward = data.get('link_distributions', {})
            link_dists_backward = link_dists_forward
        
        return adj_forward, adj_backward, link_dists_forward, link_dists_backward
    
    except Exception as e: 
        print(f"  âœ— åŠ è½½å¤±è´¥:  {e}")
        import traceback
        traceback.print_exc()
        print(f"{'='*70}\n")
        return None


def get_precomputed_data():
    """
    âœ¨ è·å–é¢„è®¡ç®—çš„æ•°æ®ç»“æ„ï¼ˆä»å…¨å±€å˜é‡ï¼‰
    
    Returns:
        tuple: (adj_list_forward, adj_list_backward, link_distributions_forward, link_distributions_backward)
    """
    global LINK_DISTRIBUTIONS_BACKWARD_GLOBAL  # ç¡®ä¿å£°æ˜äº†è¿™ä¸ªå…¨å±€å˜é‡
    
    if not DATA_LOADED:
        raise RuntimeError("æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_data_once()")
    
    # å…¼å®¹æ€§ï¼šå¦‚æœåå‘åˆ†å¸ƒæœªå®šä¹‰ï¼Œä½¿ç”¨æ­£å‘åˆ†å¸ƒ
    if LINK_DISTRIBUTIONS_BACKWARD_GLOBAL is None:
        print("  âš ï¸ åå‘é“¾è·¯åˆ†å¸ƒæœªå®šä¹‰ï¼Œä½¿ç”¨æ­£å‘åˆ†å¸ƒ")
        LINK_DISTRIBUTIONS_BACKWARD_GLOBAL = LINK_DISTRIBUTIONS_GLOBAL
    
    return (ADJ_LIST_FORWARD_GLOBAL, ADJ_LIST_BACKWARD_GLOBAL, 
            LINK_DISTRIBUTIONS_GLOBAL, LINK_DISTRIBUTIONS_BACKWARD_GLOBAL)


# def load_data_once(data_path=None):
#     """
#     å…¨å±€åŠ è½½æ•°æ®ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
#     âœ¨ æ–°å¢ï¼šåŒæ—¶é¢„è®¡ç®—é‚»æ¥è¡¨å’Œé“¾è·¯åˆ†å¸ƒ
#     """
#     global G_GLOBAL, SPARSE_DATA_GLOBAL, NODE_TO_INDEX_GLOBAL
#     global SCENARIO_DATES_GLOBAL, SCENARIO_PROBS_GLOBAL, TIME_INTERVALS_PER_DAY_GLOBAL
#     global ADJ_LIST_FORWARD_GLOBAL, ADJ_LIST_BACKWARD_GLOBAL, LINK_DISTRIBUTIONS_GLOBAL
#     global DATA_LOADED
    
#     if DATA_LOADED:
#         print("æ•°æ®å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½")
#         return (G_GLOBAL, SPARSE_DATA_GLOBAL, NODE_TO_INDEX_GLOBAL,
#                 SCENARIO_DATES_GLOBAL, SCENARIO_PROBS_GLOBAL, TIME_INTERVALS_PER_DAY_GLOBAL)
    
#     # å¯¼å…¥config
#     try:
#         import config as config
#         if data_path is None:
#             data_path = config.DATA_PATH
#     except: 
#         if data_path is None:
#             data_path = 'data/test_data.pkl.gz'
    
#     print(f"\n{'='*70}")
#     print(f"åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆ - åªæ‰§è¡Œä¸€æ¬¡ï¼‰")
#     print(f"{'='*70}")
#     print(f"  æ•°æ®æ–‡ä»¶: {data_path}")
    
#     if not os.path.exists(data_path):
#         raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
#     start_time = time_module.time()
    
#     # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#     # æ­¥éª¤1ï¼šåŠ è½½åŸºç¡€æ•°æ®
#     # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#     print(f"\n[1/4] åŠ è½½åŸºç¡€æ•°æ®...")
#     with gzip.open(data_path, 'rb') as f:
#         data = pickle.load(f)
    
#     G_GLOBAL = data['G']
#     SPARSE_DATA_GLOBAL = data['sparse_data']
#     NODE_TO_INDEX_GLOBAL = data['node_to_index']
#     SCENARIO_DATES_GLOBAL = [datetime.strptime(d, '%Y-%m-%d').date() 
#                               for d in data['scenario_dates']]
#     SCENARIO_PROBS_GLOBAL = data['scenario_probs']
#     TIME_INTERVALS_PER_DAY_GLOBAL = data['time_intervals_per_day']
    
#     print(f"  âœ“ åŸºç¡€æ•°æ®åŠ è½½å®Œæˆ")
#     print(f"    èŠ‚ç‚¹æ•°: {len(G_GLOBAL.nodes()):,}")
#     print(f"    è¾¹æ•°: {len(G_GLOBAL.edges()):,}")
#     print(f"    åœºæ™¯æ•°: {len(SCENARIO_DATES_GLOBAL)}")
#     print(f"    æ—¶é—´ç‰‡æ•°/å¤©: {TIME_INTERVALS_PER_DAY_GLOBAL}")
    
#     # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#     # æ­¥éª¤2ï¼šæ„å»ºé‚»æ¥è¡¨
#     # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#     print(f"\n[2/4] æ„å»ºé‚»æ¥è¡¨...")
#     adj_start = time_module.time()
    
#     ADJ_LIST_FORWARD_GLOBAL, ADJ_LIST_BACKWARD_GLOBAL = _build_adjacency_lists(
#         SPARSE_DATA_GLOBAL,
#         NODE_TO_INDEX_GLOBAL,
#         len(SCENARIO_DATES_GLOBAL)
#     )
    
#     adj_time = time_module.time() - adj_start
#     print(f"  âœ“ é‚»æ¥è¡¨æ„å»ºå®Œæˆ (ç”¨æ—¶ {adj_time:.2f}ç§’)")
#     print(f"    æ­£å‘é‚»æ¥è¡¨: {len(ADJ_LIST_FORWARD_GLOBAL)} ä¸ªèŠ‚ç‚¹")
#     print(f"    åå‘é‚»æ¥è¡¨: {len(ADJ_LIST_BACKWARD_GLOBAL)} ä¸ªèŠ‚ç‚¹")
    
#     # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#     # æ­¥éª¤3ï¼šé¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ
#     # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#     print(f"\n[3/4] é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ...")
#     dist_start = time_module.time()
    
#     LINK_DISTRIBUTIONS_GLOBAL = _precompute_link_distributions(
#         SPARSE_DATA_GLOBAL,
#         NODE_TO_INDEX_GLOBAL,
#         len(SCENARIO_DATES_GLOBAL)
#     )
    
#     dist_time = time_module.time() - dist_start
#     print(f"  âœ“ é“¾è·¯åˆ†å¸ƒè®¡ç®—å®Œæˆ (ç”¨æ—¶ {dist_time:.2f}ç§’)")
#     print(f"    é“¾è·¯åˆ†å¸ƒæ•°: {len(LINK_DISTRIBUTIONS_GLOBAL):,}")
    
#     # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#     # æ­¥éª¤4ï¼šå®Œæˆ
#     # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#     load_time = time_module.time() - start_time
    
#     print(f"\n[4/4] âœ“ æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å®Œæˆï¼")
#     print(f"  æ€»è€—æ—¶: {load_time:.2f}ç§’")
#     print(f"    åŸºç¡€æ•°æ®åŠ è½½: {load_time - adj_time - dist_time:.2f}ç§’")
#     print(f"    é‚»æ¥è¡¨æ„å»º: {adj_time:.2f}ç§’")
#     print(f"    é“¾è·¯åˆ†å¸ƒè®¡ç®—: {dist_time:.2f}ç§’")
#     print(f"{'='*70}\n")
    
#     DATA_LOADED = True
    
#     return (G_GLOBAL, SPARSE_DATA_GLOBAL, NODE_TO_INDEX_GLOBAL,
#             SCENARIO_DATES_GLOBAL, SCENARIO_PROBS_GLOBAL, TIME_INTERVALS_PER_DAY_GLOBAL)


def _build_adjacency_lists(sparse_data:  Dict, node_to_index: Dict, n_scenarios: int) -> Tuple[Dict, Dict]:
    """
    æ„å»ºæ­£å‘å’Œåå‘é‚»æ¥è¡¨
    
    Args:
        sparse_data: ç¨€ç–æ—…è¡Œæ—¶é—´æ•°æ®
        node_to_index: èŠ‚ç‚¹åˆ°ç´¢å¼•çš„æ˜ å°„
        n_scenarios: åœºæ™¯æ•°é‡
    
    Returns:
        adj_list_forward: æ­£å‘é‚»æ¥è¡¨ {from_node: [to_node1, to_node2, ...]}
        adj_list_backward: åå‘é‚»æ¥è¡¨ {to_node: [from_node1, from_node2, ...]}
    """
    index_to_node = {v:  k for k, v in node_to_index.items()}
    
    # æå–å”¯ä¸€è¾¹ï¼ˆå»é‡ï¼‰
    edges_set = set()
    for (scenario_idx, time_idx, from_idx, to_idx) in sparse_data.keys():
        if scenario_idx < n_scenarios:
            from_node = index_to_node[from_idx]
            to_node = index_to_node[to_idx]
            edges_set.add((from_node, to_node))
    
    # æ„å»ºé‚»æ¥è¡¨
    adj_list_forward = defaultdict(list)
    adj_list_backward = defaultdict(list)
    
    for from_node, to_node in edges_set:
        adj_list_forward[from_node].append(to_node)
        adj_list_backward[to_node].append(from_node)
    
    # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
    return dict(adj_list_forward), dict(adj_list_backward)

def _precompute_link_distributions(sparse_data: Dict, node_to_index: Dict, n_scenarios: int) -> tuple: 
    """
    é¢„è®¡ç®—æ‰€æœ‰é“¾è·¯çš„æ—…è¡Œæ—¶é—´åˆ†å¸ƒï¼ˆåŒæ—¶ç”Ÿæˆæ­£å‘å’Œåå‘ç‰ˆæœ¬ï¼‰
    
    Args: 
        sparse_data: ç¨€ç–æ—…è¡Œæ—¶é—´æ•°æ®
        node_to_index: èŠ‚ç‚¹åˆ°ç´¢å¼•çš„æ˜ å°„
        n_scenarios: åœºæ™¯æ•°é‡
    
    Returns: 
        tuple: (link_distributions_forward, link_distributions_backward)
            - link_distributions_forward: æ­£å‘æ±‚è§£ç”¨ï¼Œ{(from_node, to_node, time_slot): LinkTimeDistribution}
            - link_distributions_backward: åå‘æ±‚è§£ç”¨ï¼Œ{(from_node, to_node, time_slot): LinkTimeDistribution}
    """
    print(f"    åŒæ—¶è®¡ç®—æ­£å‘å’Œåå‘é“¾è·¯åˆ†å¸ƒ...")
    start_time = time_module.time()
    
    # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
    try:
        from forward_solver import LinkTimeDistribution as ForwardLinkDist
        forward_available = True
    except ImportError: 
        print(f"      âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥ forward_solver.LinkTimeDistribution")
        forward_available = False
    
    try:
        from reverse_solver_pseudocode import LinkTimeDistribution as ReverseLinkDist
        reverse_available = True
    except ImportError:
        print(f"      âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥ reverse_solver_pseudocode.LinkTimeDistribution")
        reverse_available = False
    
    if not forward_available and not reverse_available:
        raise ImportError("æ— æ³•å¯¼å…¥ä»»ä½• LinkTimeDistribution ç±»")
    
    index_to_node = {v:  k for k, v in node_to_index.items()}
    
    # æ”¶é›†æ¯æ¡é“¾è·¯åœ¨æ¯ä¸ªæ—¶é—´ç‰‡çš„æ—…è¡Œæ—¶é—´
    link_time_data = defaultdict(list)
    
    for (scenario_idx, time_idx, from_idx, to_idx), travel_time_minutes in sparse_data.items():
        if scenario_idx >= n_scenarios: 
            continue
        
        from_node = index_to_node[from_idx]
        to_node = index_to_node[to_idx]
        travel_time_01min = int(travel_time_minutes * 10)  # è½¬æ¢ä¸º0.1åˆ†é’Ÿå•ä½
        
        link_time_data[(from_node, to_node, time_idx)].append(travel_time_01min)
    
    # è®¡ç®—åˆ†å¸ƒ
    link_distributions_forward = {}
    link_distributions_backward = {}
    distribution_count = 0
    skipped_count = 0
    
    for (u, v, t), times in link_time_data.items():
        # ç»Ÿè®¡é¢‘ç‡
        time_counts = defaultdict(int)
        for time_val in times:
            time_counts[time_val] += 1
        
        # è®¡ç®—æ¦‚ç‡
        total = len(times)
        time_prob = {time_val:  count/total for time_val, count in time_counts.items()}
        
        # åˆ›å»ºæ­£å‘åˆ†å¸ƒå¯¹è±¡
        if forward_available:
            try: 
                link_distributions_forward[(u, v, t)] = ForwardLinkDist(time_prob, time_slot=t)
                distribution_count += 1
            except (ValueError, Exception) as e:
                skipped_count += 1
        
        # åˆ›å»ºåå‘åˆ†å¸ƒå¯¹è±¡
        if reverse_available:
            try: 
                link_distributions_backward[(u, v, t)] = ReverseLinkDist(time_prob, time_slot=t)
            except (ValueError, Exception) as e:
                pass
    
    elapsed = time_module.time() - start_time
    
    print(f"      âœ“ å®Œæˆ (ç”¨æ—¶ {elapsed:.2f}s)")
    print(f"        æ­£å‘åˆ†å¸ƒ:  {len(link_distributions_forward):,} ä¸ª")
    print(f"        åå‘åˆ†å¸ƒ: {len(link_distributions_backward):,} ä¸ª")
    if skipped_count > 0:
        print(f"        è·³è¿‡æ— æ•ˆ:  {skipped_count} ä¸ª")
    
    return link_distributions_forward, link_distributions_backward


# def _precompute_link_distributions(sparse_data: Dict, node_to_index: Dict, n_scenarios: int) -> Dict:
#     """
#     é¢„è®¡ç®—æ‰€æœ‰é“¾è·¯çš„æ—…è¡Œæ—¶é—´åˆ†å¸ƒ
    
#     Args:
#         sparse_data: ç¨€ç–æ—…è¡Œæ—¶é—´æ•°æ®
#         node_to_index: èŠ‚ç‚¹åˆ°ç´¢å¼•çš„æ˜ å°„
#         n_scenarios: åœºæ™¯æ•°é‡
    
#     Returns: 
#         link_distributions: {(from_node, to_node, time_slot): LinkTimeDistribution}
#     """
#     from forward_solver import LinkTimeDistribution

#     from reverse_solver_pseudocode import LinkTimeDistribution as LinkTimeDistributionbw
    
#     index_to_node = {v:  k for k, v in node_to_index.items()}
    
#     # æ”¶é›†æ¯æ¡é“¾è·¯åœ¨æ¯ä¸ªæ—¶é—´ç‰‡çš„æ—…è¡Œæ—¶é—´
#     link_time_data = defaultdict(list)
    
#     for (scenario_idx, time_idx, from_idx, to_idx), travel_time_minutes in sparse_data.items():
#         if scenario_idx >= n_scenarios:
#             continue
        
#         from_node = index_to_node[from_idx]
#         to_node = index_to_node[to_idx]
#         travel_time_01min = int(travel_time_minutes * 10)  # è½¬æ¢ä¸º0.1åˆ†é’Ÿå•ä½
        
#         link_time_data[(from_node, to_node, time_idx)].append(travel_time_01min)
    
#     # è®¡ç®—åˆ†å¸ƒ
#     link_distributions = {}
#     distribution_count = 0
#     skipped_count = 0
    
#     for (u, v, t), times in link_time_data.items():
#         # ç»Ÿè®¡é¢‘ç‡
#         time_counts = defaultdict(int)
#         for time_val in times:
#             time_counts[time_val] += 1
        
#         # è®¡ç®—æ¦‚ç‡
#         total = len(times)
#         time_prob = {time_val: count/total for time_val, count in time_counts.items()}
        
#         # åˆ›å»ºåˆ†å¸ƒå¯¹è±¡
#         try:
#             link_distributions[(u, v, t)] = LinkTimeDistribution(time_prob, time_slot=t)
#             distribution_count += 1
#         except ValueError:
#             # åˆ†å¸ƒæ— æ•ˆï¼ˆå¦‚å…¨é›¶ï¼‰ï¼Œè·³è¿‡
#             skipped_count += 1
#             continue
    
#     if skipped_count > 0:
#         print(f"    (è·³è¿‡ {skipped_count} ä¸ªæ— æ•ˆåˆ†å¸ƒ)")
    
#     return link_distributions


def get_data():
    """è·å–å…¨å±€åŸºç¡€æ•°æ®"""
    if not DATA_LOADED:
        return load_data_once()
    return (G_GLOBAL, SPARSE_DATA_GLOBAL, NODE_TO_INDEX_GLOBAL,
            SCENARIO_DATES_GLOBAL, SCENARIO_PROBS_GLOBAL, TIME_INTERVALS_PER_DAY_GLOBAL)


# def get_precomputed_data():
#     """
#     âœ¨ æ–°å¢ï¼šè·å–é¢„è®¡ç®—çš„æ•°æ®ç»“æ„
    
#     Returns:
#         adj_list_forward: æ­£å‘é‚»æ¥è¡¨
#         adj_list_backward: åå‘é‚»æ¥è¡¨
#         link_distributions: é“¾è·¯åˆ†å¸ƒ
#     """
#     if not DATA_LOADED:
#         raise RuntimeError("æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_data_once()")
    
#     return ADJ_LIST_FORWARD_GLOBAL, ADJ_LIST_BACKWARD_GLOBAL, LINK_DISTRIBUTIONS_GLOBAL


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¿ç•™åŸæœ‰çš„è¾…åŠ©å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def select_od_pair(node_to_index):
    """é€‰æ‹©ODå¯¹ï¼ˆä½¿ç”¨æŒ‡å®šç§å­ï¼‰"""
    nodes = list(node_to_index.keys())
    np.random.seed(int(time_module.time()))
    origin = np.random.choice(nodes)
    destination = np.random.choice([n for n in nodes if n != origin])
    return origin, destination


def time_to_string(time_01min):
    """å°†0.1åˆ†é’Ÿå•ä½è½¬æ¢ä¸ºHH:MMæ ¼å¼"""
    total_minutes = time_01min / 10
    hours = int(total_minutes // 60)
    minutes = int(total_minutes % 60)
    return f"{hours:02d}:{minutes:02d}"


def format_minutes(time_01min):
    """æ ¼å¼åŒ–åˆ†é’Ÿæ•°ï¼ˆå¸¦å•ä½ï¼‰"""
    minutes = time_01min / 10
    return f"{minutes:.1f}åˆ†é’Ÿ"


def format_path(path):
    """æ ¼å¼åŒ–è·¯å¾„è¾“å‡º"""
    if len(path) <= 10:
        return ' â†’ '.join(map(str, path))
    else:
        return (f"{' â†’ '.join(map(str, path[:5]))} â†’ ..."
                f"â†’ {' â†’ '.join(map(str, path[-3:]))}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æµ‹è¯•1: åŸºæœ¬æ±‚è§£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_1_basic_solve():
    """æµ‹è¯•1: åŸºæœ¬æ±‚è§£"""
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•1: åŸºæœ¬æ±‚è§£")
    print(f"{'='*70}\n")
    
    # è·å–å…¨å±€æ•°æ®
    G, sparse_data, node_to_index, scenario_dates, scenario_probs, time_intervals_per_day = get_data()
    
    # åˆå§‹åŒ–æ±‚è§£å™¨
    mode = config.get_mode_config('standard')
    
    solver = ReverseLabelSettingSolver(
        G, sparse_data, node_to_index, scenario_dates,
        scenario_probs, time_intervals_per_day,
        L1=mode['L1'],
        L2=mode['L2'],
        verbose=config.REVERSE_VERBOSE
    )
    
    # é€‰æ‹©ODå¯¹ï¼ˆä½¿ç”¨æµ‹è¯•1çš„ç§å­ï¼‰
    origin, destination = select_od_pair(node_to_index)
    print(f"  æµ‹è¯•ODå¯¹ (seed=1001): {origin} â†’ {destination}")
    
    # è®¾ç½®é—®é¢˜å‚æ•°
    target_arrival_time = (config.DEFAULT_ARRIVAL_HOUR * 60 + 
                          config.DEFAULT_ARRIVAL_MINUTE) * 10
    alpha = config.REVERSE_ALPHA_DEFAULT
    
    print(f"  ç›®æ ‡åˆ°è¾¾æ—¶é—´: {time_to_string(target_arrival_time)}")
    print(f"  å¯é æ€§è¦æ±‚: Î±={alpha}\n")
    
    # æ±‚è§£
    result = solver.solve(
        origin=origin,
        destination=destination,
        target_arrival_time=target_arrival_time,
        alpha=alpha,
        max_labels=mode['max_labels']
    )
    
    # éªŒè¯ç»“æœ
    print(f"\n{'â”€'*70}")
    print(f"æµ‹è¯•1éªŒè¯")
    print(f"{'â”€'*70}")
    
    assert result['success'], "âŒ æ±‚è§£å¤±è´¥"
    print(f"  âœ“ æ±‚è§£æˆåŠŸ")
    
    assert result['path'] is not None, "âŒ è·¯å¾„ä¸ºç©º"
    print(f"  âœ“ è·¯å¾„éç©º (é•¿åº¦: {len(result['path'])})")
    
    assert result['path'][0] == origin, "âŒ èµ·ç‚¹ä¸åŒ¹é…"
    print(f"  âœ“ èµ·ç‚¹æ­£ç¡®: {origin}")
    
    assert result['path'][-1] == destination, "âŒ ç»ˆç‚¹ä¸åŒ¹é…"
    print(f"  âœ“ ç»ˆç‚¹æ­£ç¡®: {destination}")
    
    assert result['latest_departure_time'] > 0, "âŒ æœ€æ™šå‡ºå‘æ—¶é—´æ— æ•ˆ"
    print(f"  âœ“ æœ€æ™šå‡ºå‘æ—¶é—´: {time_to_string(result['latest_departure_time'])}")
    
    assert result['reserved_time'] > 0, "âŒ é¢„ç•™æ—¶é—´æ— æ•ˆ"
    print(f"  âœ“ é¢„ç•™æ—¶é—´: {result['reserved_time']/10:.1f}åˆ†é’Ÿ")
    
    assert result['latest_departure_time'] < target_arrival_time, "âŒ å‡ºå‘æ™šäºåˆ°è¾¾"
    print(f"  âœ“ æ—¶é—´é€»è¾‘æ­£ç¡®")
    
    print(f"\n  ğŸ‰ æµ‹è¯•1é€šè¿‡ï¼")
    print(f"{'='*70}\n")
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æµ‹è¯•2: Î±æ•æ„Ÿæ€§åˆ†æï¼ˆå®Œæ•´ç‰ˆï¼š0.05-0.95ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_2_alpha_sensitivity():
    """æµ‹è¯•2: Î±æ•æ„Ÿæ€§åˆ†æï¼ˆ0.05-0.95ï¼‰"""
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•2: Î±æ•æ„Ÿæ€§åˆ†æï¼ˆå®Œæ•´ç‰ˆï¼‰")
    print(f"{'='*70}\n")
    
    # è·å–å…¨å±€æ•°æ®
    G, sparse_data, node_to_index, scenario_dates, scenario_probs, time_intervals_per_day = get_data()
    
    mode = config.get_mode_config('fast')
    
    solver = ReverseLabelSettingSolver(
        G, sparse_data, node_to_index, scenario_dates,
        scenario_probs, time_intervals_per_day,
        L1=mode['L1'],
        L2=mode['L2'],
        verbose=False
    )
    
    # ä½¿ç”¨æµ‹è¯•2çš„ç§å­
    origin, destination = select_od_pair(node_to_index)
    target_arrival_time = 9 * 60 * 10  # 09:00
    
    print(f"  æµ‹è¯•ODå¯¹ (seed=2002): {origin} â†’ {destination}")
    print(f"  ç›®æ ‡åˆ°è¾¾æ—¶é—´: {time_to_string(target_arrival_time)}\n")
    
    # å®Œæ•´çš„Î±å€¼èŒƒå›´ï¼š0.05, 0.10, 0.15, ..., 0.95
    alphas = config.ALPHA_SENSITIVITY_VALUES
    
    print(f"  æµ‹è¯•Î±å€¼èŒƒå›´: 0.05 åˆ° 0.95 (æ­¥é•¿0.05)")
    print(f"  æ€»å…± {len(alphas)} ä¸ªæµ‹è¯•ç‚¹\n")
    
    results = []
    # detailed_alphas = [0.50, 0.75, 0.95]  # ä¸­ã€é«˜ã€å¾ˆé«˜å¯é æ€§
    detailed_alphas = alphas
    detailed_results = {}
    
    print(f"  å¼€å§‹æµ‹è¯•:")
    for i, alpha in enumerate(alphas, 1):
        print(f"    [{i:2d}/{len(alphas)}] Î±={alpha:.2f}...", end='', flush=True)
        
        save_all = alpha in detailed_alphas

        result = solver.solve(
            origin, destination, target_arrival_time, alpha,
            max_labels=mode['max_labels']
        )
        
        if result['success']:
            result_data = {
                'alpha': alpha,
                'latest_departure': result['latest_departure_time'],
                'expected_departure': result['expected_departure_time'],
                'reserved_time': result['reserved_time'],
                'path': result['path'],
                'path_length': len(result['path']),
                'target_arrival': target_arrival_time,
                'distribution': result['distribution']  # â† ä¿å­˜åˆ†å¸ƒç”¨äºå¯è§†åŒ–
            }
            
            # âœ… å¦‚æœä¿å­˜äº†æ‰€æœ‰è·¯å¾„ï¼Œæ·»åŠ åˆ°è¯¦ç»†ç»“æœ
            if save_all and 'all_paths' in result:
                result_data['all_paths'] = result['all_paths']
                result_data['num_candidates'] = result['num_candidate_paths']
                detailed_results[alpha] = result_data
                print(f" âœ“ æœ€æ™š={time_to_string(result['latest_departure_time'])}, "
                      f"é¢„ç•™={result['reserved_time']/10:.1f}åˆ†, "
                      f"å€™é€‰è·¯å¾„={result['num_candidate_paths']}")
            else:
                print(f" âœ“ æœ€æ™š={time_to_string(result['latest_departure_time'])}, "
                      f"é¢„ç•™={result['reserved_time']/10:.1f}åˆ†")
            
            results.append(result_data)

                # ç»˜åˆ¶æ‰€æœ‰å€™é€‰è·¯å¾„çš„åˆ†å¸ƒå¯¹æ¯”
            # plot_all_paths_distributions(
            #     result, 
            #     analysis_alpha, 
            #     target_arrival_time,
            #     output_file=f'result/alpha_{int(analysis_alpha*100)}_all_paths.png'
            # )
        else:
            print(f" âœ— å¤±è´¥")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # éªŒè¯
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    print(f"\n{'â”€'*70}")
    print(f"æµ‹è¯•2éªŒè¯")
    print(f"{'â”€'*70}")
    
    success_rate = len(results) / len(alphas) * 100
    print(f"  æˆåŠŸç‡: {len(results)}/{len(alphas)} ({success_rate:.1f}%)")
    
    assert len(results) >= len(alphas) * 0.7, \
        f"âŒ æˆåŠŸæ±‚è§£çš„Î±å€¼å¤ªå°‘: {len(results)}/{len(alphas)}"
    print(f"  âœ“ æˆåŠŸç‡è¾¾æ ‡ (â‰¥70%)")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # æ‰“å°è¯¦ç»†å¯¹æ¯”è¡¨ï¼ˆå…¨éƒ¨ä½¿ç”¨HH:MMæ ¼å¼ï¼‰
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    print(f"\n  Î±æ•æ„Ÿæ€§è¯¦ç»†å¯¹æ¯”:")
    print(f"  {'Î±å€¼':<8} {'æœ€æ™šå‡ºå‘':<12} {'æœŸæœ›å‡ºå‘':<12} {'ç›®æ ‡åˆ°è¾¾':<12} "
          f"{'é¢„ç•™(åˆ†)':<12} {'è·¯å¾„é•¿åº¦':<10}")
    print(f"  {'-'*80}")
    
    # æ˜¾ç¤ºæ‰€æœ‰ç»“æœï¼ˆæˆ–éƒ¨åˆ†å…³é”®ç‚¹ï¼‰
    display_all = len(results) <= 10
    
    if display_all:
        for r in results:
            print(f"  {r['alpha']:<8.2f} "
                  f"{time_to_string(r['latest_departure']):<12} "
                  f"{time_to_string(r['expected_departure']):<12} "
                  f"{time_to_string(r['target_arrival']):<12} "
                  f"{r['reserved_time']/10:<12.1f} "
                  f"{r['path_length']:<10}")
    else:
        # æ˜¾ç¤ºå…³é”®ç‚¹
        key_indices = [0, len(results)//4, len(results)//2, 3*len(results)//4, -1]
        for i in key_indices:
            if i < len(results):
                r = results[i]
                print(f"  {r['alpha']:<8.2f} "
                      f"{time_to_string(r['latest_departure']):<12} "
                      f"{time_to_string(r['expected_departure']):<12} "
                      f"{time_to_string(r['target_arrival']):<12} "
                      f"{r['reserved_time']/10:<12.1f} "
                      f"{r['path_length']:<10}")
        print(f"  ...(æ˜¾ç¤º {len(key_indices)}/{len(results)} ä¸ªç»“æœï¼Œå®Œæ•´ç»“æœè§è¾“å‡ºæ–‡ä»¶)")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # æ‰“å°è·¯å¾„è¯¦æƒ…ï¼ˆé€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§çš„Î±å€¼ï¼‰
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    print(f"\n  è·¯å¾„è¯¦æƒ…ï¼ˆä»£è¡¨æ€§Î±å€¼ï¼‰:")
    print(f"  {'-'*70}")
    
    # é€‰æ‹©3ä¸ªä»£è¡¨æ€§Î±å€¼ï¼šä½ã€ä¸­ã€é«˜
    representative_indices = []
    if len(results) > 0:
        representative_indices.append(0)
    if len(results) >= 2:
        representative_indices.append(len(results)//2)
    if len(results) >= 3:
        representative_indices.append(-1)
    
    for idx in representative_indices:
        if idx < len(results):
            r = results[idx]
            print(f"\n  ã€Î± = {r['alpha']:.2f}ã€‘")
            print(f"    èµ·ç‚¹: {origin}")
            print(f"    ç»ˆç‚¹: {destination}")
            print(f"    è·¯å¾„: {format_path(r['path'])}")
            print(f"    è·¯å¾„é•¿åº¦: {r['path_length']} ä¸ªèŠ‚ç‚¹")
            print(f"    â”Œâ”€ æ—¶é—´ä¿¡æ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            print(f"    â”‚ æœ€æ™šå‡ºå‘æ—¶é—´: {time_to_string(r['latest_departure']):<10} "
                  f"({format_minutes(r['latest_departure'])})  â”‚")
            print(f"    â”‚ æœŸæœ›å‡ºå‘æ—¶é—´: {time_to_string(r['expected_departure']):<10} "
                  f"({format_minutes(r['expected_departure'])})  â”‚")
            print(f"    â”‚ ç›®æ ‡åˆ°è¾¾æ—¶é—´: {time_to_string(r['target_arrival']):<10} "
                  f"({format_minutes(r['target_arrival'])})  â”‚")
            print(f"    â”‚ é¢„ç•™æ—¶é—´:     {r['reserved_time']/10:>6.1f} åˆ†é’Ÿ"
                  f"{' '*26}â”‚")
            print(f"    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # éªŒè¯å•è°ƒæ€§
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    print(f"\n  å•è°ƒæ€§æ£€æŸ¥ (æŠ½æ ·éªŒè¯):")
    monotonic_violations = 0
    check_indices = [i for i in range(len(results)-1) if i % 3 == 0]
    
    for i in check_indices:
        curr = results[i]
        next_r = results[i+1]
        
        # Î±å¢å¤§æ—¶ï¼Œæœ€æ™šå‡ºå‘æ—¶é—´åº”è¯¥å‡å°æˆ–ç›¸è¿‘ï¼ˆå®¹å·®10åˆ†é’Ÿï¼‰
        if curr['latest_departure'] < next_r['latest_departure'] - 100:
            monotonic_violations += 1
            print(f"    âš  Î±={curr['alpha']:.2f}â†’{next_r['alpha']:.2f}: "
                  f"æœ€æ™šå‡ºå‘åè€Œå¢åŠ  "
                  f"({time_to_string(curr['latest_departure'])} â†’ "
                  f"{time_to_string(next_r['latest_departure'])})")
    
    if monotonic_violations == 0:
        print(f"    âœ“ æ‰€æœ‰æ£€æŸ¥ç‚¹ç¬¦åˆå•è°ƒæ€§")
    else:
        print(f"    âš  {monotonic_violations}/{len(check_indices)} ä¸ªç‚¹è¿åå•è°ƒæ€§ "
              f"({monotonic_violations/len(check_indices)*100:.1f}%)")
    
    try:
        save_alpha_sensitivity_results(results, origin, destination, target_arrival_time)
        print(f"\n  âœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: alpha_sensitivity_results.txt")
    except Exception as e:
        print(f"\n  âš  ä¿å­˜ç»“æœå¤±è´¥: {e}")
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # å¯è§†åŒ–
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if config.SHOW_PLOTS and len(results) >= 5:
        try:
            plot_alpha_sensitivity(results, target_arrival_time)
        except Exception as e:
            print(f"    âš  å¯è§†åŒ–å¤±è´¥: {e}")
    
    print(f"\n  ğŸ‰ æµ‹è¯•2é€šè¿‡ï¼")
    print(f"{'='*70}\n")
    
    return {
        'all_results': results,
        'detailed_results': detailed_results,  # â† æ–°å¢ï¼šåŒ…å«æ‰€æœ‰å€™é€‰è·¯å¾„çš„è¯¦ç»†ç»“æœ
        'origin': origin,
        'destination': destination,
        'target_arrival_time': target_arrival_time
    }

def save_alpha_sensitivity_results(results, origin, destination, target_arrival_time):
    """ä¿å­˜Î±æ•æ„Ÿæ€§åˆ†æè¯¦ç»†ç»“æœåˆ°æ–‡ä»¶ï¼ˆå…¨éƒ¨ä½¿ç”¨HH:MMæ ¼å¼ï¼‰"""
    filename = 'result/alpha_sensitivity_results.txt'
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("="*90 + "\n")
        f.write("Î±æ•æ„Ÿæ€§åˆ†æè¯¦ç»†ç»“æœ\n")
        f.write("="*90 + "\n\n")
        
        f.write(f"èµ·ç‚¹: {origin}\n")
        f.write(f"ç»ˆç‚¹: {destination}\n")
        f.write(f"ç›®æ ‡åˆ°è¾¾æ—¶é—´: {time_to_string(target_arrival_time)} "
                f"({format_minutes(target_arrival_time)})\n")
        f.write(f"æµ‹è¯•Î±å€¼æ•°é‡: {len(results)}\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("\n" + "="*90 + "\n\n")
        
        # æ±‡æ€»è¡¨æ ¼
        f.write("ã€æ±‡æ€»è¡¨æ ¼ã€‘\n")
        f.write("-"*90 + "\n")
        f.write(f"{'Î±å€¼':<8} {'æœ€æ™šå‡ºå‘':<12} {'æœŸæœ›å‡ºå‘':<12} {'ç›®æ ‡åˆ°è¾¾':<12} "
                f"{'é¢„ç•™(åˆ†)':<12} {'è·¯å¾„é•¿åº¦':<10}\n")
        f.write("-"*90 + "\n")
        
        for r in results:
            f.write(f"{r['alpha']:<8.2f} "
                    f"{time_to_string(r['latest_departure']):<12} "
                    f"{time_to_string(r['expected_departure']):<12} "
                    f"{time_to_string(r['target_arrival']):<12} "
                    f"{r['reserved_time']/10:<12.1f} "
                    f"{r['path_length']:<10}\n")
        
        # è¯¦ç»†è·¯å¾„ä¿¡æ¯
        f.write("\n" + "="*90 + "\n")
        f.write("ã€è¯¦ç»†è·¯å¾„ä¿¡æ¯ã€‘\n")
        f.write("="*90 + "\n")
        
        for r in results:
            f.write(f"\n{'â”€'*90}\n")
            f.write(f"Î± = {r['alpha']:.2f}\n")
            f.write(f"{'â”€'*90}\n")
            
            f.write(f"è·¯å¾„æ‘˜è¦: {format_path(r['path'])}\n")
            f.write(f"å®Œæ•´è·¯å¾„: {' â†’ '.join(map(str, r['path']))}\n")
            f.write(f"è·¯å¾„é•¿åº¦: {r['path_length']} ä¸ªèŠ‚ç‚¹\n\n")
            
            f.write(f"æ—¶é—´ä¿¡æ¯:\n")
            f.write(f"  æœ€æ™šå‡ºå‘æ—¶é—´: {time_to_string(r['latest_departure'])} "
                    f"({format_minutes(r['latest_departure'])})\n")
            f.write(f"  æœŸæœ›å‡ºå‘æ—¶é—´: {time_to_string(r['expected_departure'])} "
                    f"({format_minutes(r['expected_departure'])})\n")
            f.write(f"  ç›®æ ‡åˆ°è¾¾æ—¶é—´: {time_to_string(r['target_arrival'])} "
                    f"({format_minutes(r['target_arrival'])})\n")
            f.write(f"  é¢„ç•™æ—¶é—´:     {r['reserved_time']/10:.1f} åˆ†é’Ÿ\n")  # ä¿®æ­£è¿™é‡Œ
            
            # æ—¶é—´å·®å¼‚åˆ†æ
            time_diff = r['expected_departure'] - r['latest_departure']
            f.write(f"  å‡ºå‘æ—¶é—´å·®å¼‚: {time_diff/10:.1f} åˆ†é’Ÿ "
                    f"(æœŸæœ› - æœ€æ™š)\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        f.write("\n" + "="*90 + "\n")
        f.write("ã€ç»Ÿè®¡ä¿¡æ¯ã€‘\n")
        f.write("="*90 + "\n\n")
        
        reserved_times = [r['reserved_time']/10 for r in results]
        path_lengths = [r['path_length'] for r in results]
        
        f.write(f"é¢„ç•™æ—¶é—´ç»Ÿè®¡:\n")
        f.write(f"  æœ€å°å€¼: {min(reserved_times):.1f} åˆ†é’Ÿ (Î±={results[np.argmin(reserved_times)]['alpha']:.2f})\n")
        f.write(f"  æœ€å¤§å€¼: {max(reserved_times):.1f} åˆ†é’Ÿ (Î±={results[np.argmax(reserved_times)]['alpha']:.2f})\n")
        f.write(f"  å¹³å‡å€¼: {np.mean(reserved_times):.1f} åˆ†é’Ÿ\n")
        f.write(f"  æ ‡å‡†å·®: {np.std(reserved_times):.1f} åˆ†é’Ÿ\n\n")  # ä¿®æ­£è¿™é‡Œ
        
        f.write(f"è·¯å¾„é•¿åº¦ç»Ÿè®¡:\n")
        f.write(f"  æœ€å°å€¼: {min(path_lengths)} ä¸ªèŠ‚ç‚¹\n")
        f.write(f"  æœ€å¤§å€¼: {max(path_lengths)} ä¸ªèŠ‚ç‚¹\n")
        f.write(f"  å¹³å‡å€¼: {np.mean(path_lengths):.1f} ä¸ªèŠ‚ç‚¹\n")
        
        # å•è°ƒæ€§åˆ†æ
        f.write(f"\nå•è°ƒæ€§åˆ†æ:\n")
        violations = 0
        for i in range(len(results)-1):
            if results[i]['latest_departure'] < results[i+1]['latest_departure'] - 100:
                violations += 1
        
        f.write(f"  æ£€æŸ¥ç‚¹æ•°: {len(results)-1}\n")
        f.write(f"  è¿åå•è°ƒæ€§: {violations} ä¸ª\n")
        f.write(f"  å•è°ƒæ€§ç‡: {(1-violations/(len(results)-1))*100:.1f}%\n")

def time_to_string(time_01min):
    """
    å°†0.1åˆ†é’Ÿå•ä½è½¬æ¢ä¸ºHH:MMæ ¼å¼
    
    Args:
        time_01min: æ—¶é—´ï¼ˆ0.1åˆ†é’Ÿå•ä½ï¼‰
        
    Returns:
        HH:MMæ ¼å¼çš„å­—ç¬¦ä¸²
    """
    total_minutes = time_01min / 10
    hours = int(total_minutes // 60)
    minutes = int(total_minutes % 60)
    return f"{hours:02d}:{minutes:02d}"


def format_minutes(time_01min):
    """
    æ ¼å¼åŒ–åˆ†é’Ÿæ•°ï¼ˆå¸¦å•ä½ï¼‰
    
    Args:
        time_01min: æ—¶é—´ï¼ˆ0.1åˆ†é’Ÿå•ä½ï¼‰
        
    Returns:
        æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼Œå¦‚ "505.0åˆ†é’Ÿ"
    """
    minutes = time_01min / 10
    return f"{minutes:.1f}åˆ†é’Ÿ"


def format_path(path):
    """æ ¼å¼åŒ–è·¯å¾„è¾“å‡º"""
    if len(path) <= 10:
        return ' â†’ '.join(map(str, path))
    else:
        return (f"{' â†’ '.join(map(str, path[:5]))} â†’ ..."
                f"â†’ {' â†’ '.join(map(str, path[-3:]))}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¾…åŠ©å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def time_to_string(time_01min):
    """
    å°†0.1åˆ†é’Ÿå•ä½è½¬æ¢ä¸ºHH:MMæ ¼å¼
    
    Args:
        time_01min: æ—¶é—´ï¼ˆ0.1åˆ†é’Ÿå•ä½ï¼‰
        
    Returns:
        HH:MMæ ¼å¼çš„å­—ç¬¦ä¸²
    """
    total_minutes = time_01min / 10
    hours = int(total_minutes // 60)
    minutes = int(total_minutes % 60)
    return f"{hours:02d}:{minutes:02d}"


def plot_alpha_sensitivity(results, target_arrival_time):
    """ç»˜åˆ¶Î±æ•æ„Ÿæ€§åˆ†æå›¾"""
    alphas = [r['alpha'] for r in results]
    latest_deps = [r['latest_departure']/10 for r in results]
    reserved_times = [r['reserved_time']/10 for r in results]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # å­å›¾1: æœ€æ™šå‡ºå‘æ—¶é—´
    ax1.plot(alphas, latest_deps, 'b-o', linewidth=2, markersize=4, label='Latest Departure')
    ax1.axhline(target_arrival_time/10, color='orange', linestyle='--', 
                linewidth=2, label='Target Arrival')
    ax1.set_xlabel('Reliability Î±', fontsize=12)
    ax1.set_ylabel('Departure Time (minutes)', fontsize=12)
    ax1.set_title('Î± Sensitivity - Departure Time', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(alpha=0.3)
    
    # å­å›¾2: é¢„ç•™æ—¶é—´
    ax2.plot(alphas, reserved_times, 'r-s', linewidth=2, markersize=4, label='Reserved Time')
    ax2.set_xlabel('Reliability Î±', fontsize=12)
    ax2.set_ylabel('Reserved Time (minutes)', fontsize=12)
    ax2.set_title('Î± Sensitivity - Reserved Time', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('result/alpha_sensitivity_analysis.png', dpi=300, bbox_inches='tight')
    print(f"    âœ“ å¯è§†åŒ–å·²ä¿å­˜: alpha_sensitivity_analysis.png")
    plt.close()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æµ‹è¯•3: æ€§èƒ½æµ‹è¯•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_3_performance():
    """æµ‹è¯•3: æ€§èƒ½æµ‹è¯•"""
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•3: æ€§èƒ½æµ‹è¯•")
    print(f"{'='*70}\n")
    
    # è·å–å…¨å±€æ•°æ®
    G, sparse_data, node_to_index, scenario_dates, scenario_probs, time_intervals_per_day = get_data()
    
    # ä½¿ç”¨æµ‹è¯•3çš„ç§å­
    origin, destination = select_od_pair(node_to_index)
    target_arrival_time = 9 * 60 * 10
    
    print(f"  æµ‹è¯•ODå¯¹ (seed=3003): {origin} â†’ {destination}")
    print(f"  ç›®æ ‡åˆ°è¾¾: {time_to_string(target_arrival_time)}\n")
    
    # æµ‹è¯•ä¸åŒé…ç½®
    test_configs = [
        ('å¿«é€Ÿæ¨¡å¼', config.FAST_MODE),
        ('æ ‡å‡†æ¨¡å¼', config.STANDARD_MODE),
    ]
    
    performance_results = []
    
    for config_name, mode in test_configs:
        print(f"  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"  é…ç½®: {config_name}")
        print(f"    L1={mode['L1']}, L2={mode['L2']}, æœ€å¤§æ ‡ç­¾={mode['max_labels']:,}")
        
        solver = ReverseLabelSettingSolver(
            G, sparse_data, node_to_index, scenario_dates,
            scenario_probs, time_intervals_per_day,
            L1=mode['L1'],
            L2=mode['L2'],
            verbose=False
        )
        
        start = time_module.time()
        result = solver.solve(
            origin, destination, target_arrival_time, 0.95,
            max_labels=mode['max_labels']
        )
        elapsed = time_module.time() - start
        
        if result['success']:
            perf_data = {
                'config': config_name,
                'L1': mode['L1'],
                'L2': mode['L2'],
                'time': elapsed,
                'iterations': result['iterations'],
                'labels_generated': result['stats']['labels_generated'],
                'labels_dominated': result['stats']['labels_dominated'],
                'pruning_rate': result['stats']['labels_dominated']/result['stats']['labels_generated']*100
            }
            performance_results.append(perf_data)
            
            print(f"    âœ“ æˆåŠŸ")
            print(f"      è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"      è¿­ä»£: {result['iterations']}")
            print(f"      ç”Ÿæˆæ ‡ç­¾: {result['stats']['labels_generated']:,}")
            print(f"      å‰ªæç‡: {perf_data['pruning_rate']:.1f}%")
            print(f"      æœ€æ™šå‡ºå‘: {time_to_string(result['latest_departure_time'])}")
        else:
            print(f"    âœ— å¤±è´¥")
    
    # æ€§èƒ½å¯¹æ¯”
    if len(performance_results) >= 2:
        print(f"\n  æ€§èƒ½å¯¹æ¯”:")
        fast = performance_results[0]
        standard = performance_results[1]
        speedup = standard['time'] / fast['time']
        print(f"    å¿«é€Ÿæ¨¡å¼ vs æ ‡å‡†æ¨¡å¼:")
        print(f"      é€Ÿåº¦æå‡: {speedup:.2f}x")
        print(f"      æ ‡ç­¾æ•°å¯¹æ¯”: {fast['labels_generated']:,} vs {standard['labels_generated']:,}")
    
    print(f"\n  ğŸ‰ æµ‹è¯•3å®Œæˆï¼")
    print(f"{'='*70}\n")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æµ‹è¯•4: æ—¶é—´ä¸€è‡´æ€§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_4_time_consistency():
    """æµ‹è¯•4: æ—¶é—´ä¸€è‡´æ€§"""
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•4: æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥")
    print(f"{'='*70}\n")
    
    # è·å–å…¨å±€æ•°æ®
    G, sparse_data, node_to_index, scenario_dates, scenario_probs, time_intervals_per_day = get_data()
    
    mode = config.get_mode_config('fast')
    
    solver = ReverseLabelSettingSolver(
        G, sparse_data, node_to_index, scenario_dates,
        scenario_probs, time_intervals_per_day,
        L1=mode['L1'],
        L2=mode['L2'],
        verbose=False
    )
    
    # ä½¿ç”¨æµ‹è¯•4çš„ç§å­
    origin, destination = select_od_pair(node_to_index)
    
    print(f"  æµ‹è¯•ODå¯¹ (seed=4004): {origin} â†’ {destination}\n")
    
    # æµ‹è¯•ä¸åŒåˆ°è¾¾æ—¶é—´
    test_times = config.TIME_BUDGET_TEST_TIMES[:3]
    results = []
    
    for hour, minute in test_times:
        target_time = (hour * 60 + minute) * 10
        time_str = f"{hour:02d}:{minute:02d}"
        
        print(f"  æµ‹è¯•ç›®æ ‡åˆ°è¾¾: {time_str}...", end='', flush=True)
        
        result = solver.solve(
            origin, destination, target_time, 0.95,
            max_labels=mode['max_labels']
        )
        
        if result['success']:
            results.append({
                'target_time': target_time,
                'time_str': time_str,
                'latest_dep': result['latest_departure_time'],
                'reserved': result['reserved_time']
            })
            print(f" âœ“ æœ€æ™šå‡ºå‘={time_to_string(result['latest_departure_time'])}")
        else:
            print(f" âœ— å¤±è´¥")
    
    # éªŒè¯
    print(f"\n{'â”€'*70}")
    print(f"æµ‹è¯•4éªŒè¯")
    print(f"{'â”€'*70}")
    
    for r in results:
        # éªŒè¯æ—¶é—´é€»è¾‘
        assert r['latest_dep'] < r['target_time'], \
            f"âŒ æ—¶é—´é€»è¾‘é”™è¯¯: {r['time_str']}"
        print(f"  âœ“ {r['time_str']}: æ—¶é—´é€»è¾‘æ­£ç¡®")
        
        # éªŒè¯é¢„ç•™æ—¶é—´
        expected_reserved = r['target_time'] - r['latest_dep']
        assert abs(r['reserved'] - expected_reserved) < 1, \
            f"âŒ é¢„ç•™æ—¶é—´è®¡ç®—é”™è¯¯: {r['time_str']}"
        print(f"    é¢„ç•™æ—¶é—´: {r['reserved']/10:.1f}åˆ†")
    
    print(f"\n  ğŸ‰ æµ‹è¯•4é€šè¿‡ï¼")
    print(f"{'='*70}\n")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æµ‹è¯•5: å¤šODå¯¹æµ‹è¯•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_5_multiple_od_pairs():
    """æµ‹è¯•5: å¤šODå¯¹æµ‹è¯•ï¼ˆæµ‹è¯•ç®—æ³•ç¨³å®šæ€§ï¼‰- ä¿®æ”¹ç‰ˆ"""
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•5: å¤šODå¯¹æµ‹è¯•")
    print(f"{'='*70}\n")
    
    # è·å–å…¨å±€æ•°æ®
    G, sparse_data, node_to_index, scenario_dates, scenario_probs, time_intervals_per_day = get_data()
    
    mode = config.get_mode_config('fast')
    
    solver = ReverseLabelSettingSolver(
        G, sparse_data, node_to_index, scenario_dates,
        scenario_probs, time_intervals_per_day,
        L1=mode['L1'],
        L2=mode['L2'],
        verbose=False
    )
    
    target_arrival_time = 9 * 60 * 10
    alpha = 0.95
    
    print(f"  æµ‹è¯•å¤šä¸ªä¸åŒçš„ODå¯¹")
    print(f"  ç›®æ ‡åˆ°è¾¾: {time_to_string(target_arrival_time)}, Î±={alpha}\n")
    
    # æµ‹è¯•5å¯¹ä¸åŒçš„OD
    num_tests = config.NUM_TESTS
    success_count = 0
    results = []
    
    for i in range(num_tests):
        seed = 5000 + i
        origin, destination = select_od_pair(node_to_index)
        
        print(f"  æµ‹è¯• {i+1}/{num_tests} (seed={seed}): {origin}â†’{destination}...", 
              end='', flush=True)
        
        result = solver.solve(
            origin, destination, target_arrival_time, alpha,
            max_labels=mode['max_labels']
        )
        
        if result['success']:
            success_count += 1
            # âœ… ä¿®æ”¹ï¼šè¿”å›å®Œæ•´æ•°æ®
            results.append({
                'od': (origin, destination),
                'origin': origin,  # â† æ–°å¢
                'destination': destination,  # â† æ–°å¢
                'latest_dep': result['latest_departure_time'],
                'expected_dep': result['expected_departure_time'],  # â† æ–°å¢
                'reserved': result['reserved_time'],
                'path': result['path'],  # â† æ–°å¢
                'path_length': len(result['path']),
                'target_arrival': target_arrival_time,  # â† æ–°å¢
                'alpha': alpha,  # â† æ–°å¢
                'distribution': result['distribution']  # â† æ–°å¢ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
            })
            print(f" âœ“ é¢„ç•™={result['reserved_time']/10:.1f}åˆ†, è·¯å¾„={len(result['path'])}èŠ‚ç‚¹")
        else:
            print(f" âœ— å¤±è´¥")
    
    # éªŒè¯
    print(f"\n{'â”€'*70}")
    print(f"æµ‹è¯•5éªŒè¯")
    print(f"{'â”€'*70}")
    
    success_rate = success_count / num_tests * 100
    print(f"  æˆåŠŸç‡: {success_count}/{num_tests} ({success_rate:.1f}%)")
    
    assert success_count >= num_tests * 0.6, \
        f"âŒ æˆåŠŸç‡å¤ªä½: {success_rate:.1f}%"
    print(f"  âœ“ æˆåŠŸç‡è¾¾æ ‡ (â‰¥60%)")
    
    if results:
        print(f"\n  ç»“æœç»Ÿè®¡:")
        reserved_times = [r['reserved']/10 for r in results]
        path_lengths = [r['path_length'] for r in results]
        print(f"    é¢„ç•™æ—¶é—´: å‡å€¼={np.mean(reserved_times):.1f}åˆ†, "
              f"æ ‡å‡†å·®={np.std(reserved_times):.1f}åˆ†")
        print(f"    è·¯å¾„é•¿åº¦: å‡å€¼={np.mean(path_lengths):.1f}, "
              f"èŒƒå›´=[{min(path_lengths)}, {max(path_lengths)}]")
    
    print(f"\n  ğŸ‰ æµ‹è¯•5é€šè¿‡ï¼")
    print(f"{'='*70}\n")
    
    return results  # â† è¿”å›å®Œæ•´ç»“æœ


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print(f"\n{'='*70}")
    print(f"åå‘æ±‚è§£å™¨æµ‹è¯•å¥—ä»¶ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    print(f"{'='*70}")
    print(f"  æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  é…ç½®: L1={config.REVERSE_L1}, L2={config.REVERSE_L2}")
    print(f"  ä¼˜åŒ–: å…¨å±€åŠ è½½æ•°æ® + ç‹¬ç«‹éšæœºç§å­")
    print(f"{'='*70}\n")
    
    # é¢„å…ˆåŠ è½½æ•°æ®
    load_data_once()
    
    start_time = time_module.time()
    
    try:
        # æµ‹è¯•1: åŸºæœ¬æ±‚è§£
        test_1_basic_solve()
        
        # æµ‹è¯•2: Î±æ•æ„Ÿæ€§ï¼ˆå®Œæ•´ç‰ˆï¼‰
        test_2_alpha_sensitivity()
        
        # æµ‹è¯•3: æ€§èƒ½
        test_3_performance()
        
        # æµ‹è¯•4: æ—¶é—´ä¸€è‡´æ€§
        test_4_time_consistency()
        
        # æµ‹è¯•5: å¤šODå¯¹
        test_5_multiple_od_pairs()
        
        total_time = time_module.time() - start_time
        
        print(f"\n{'='*70}")
        print(f"æ‰€æœ‰æµ‹è¯•å®Œæˆï¼âœ“")
        print(f"{'='*70}")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"  çŠ¶æ€: å…¨éƒ¨é€šè¿‡ âœ“")
        print(f"  æ•°æ®åŠ è½½: ä»…ä¸€æ¬¡ï¼ˆä¼˜åŒ–ï¼‰")
        print(f"{'='*70}\n")
        
        return True
        
    except Exception as e:
        print(f"\n{'='*70}")
        print(f"æµ‹è¯•å¤±è´¥ï¼âœ—")
        print(f"{'='*70}")
        print(f"  é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print(f"{'='*70}\n")
        return False

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
from visualization_generator import generate_html_with_svg

def run_all_tests_with_visualization(testname: str ):
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¹¶ç”Ÿæˆå¯è§†åŒ–ï¼ˆä¿®æ”¹ç‰ˆï¼‰"""
    print(f"\n{'='*70}")
    print(f"åå‘æ±‚è§£å™¨æµ‹è¯•å¥—ä»¶ï¼ˆå¸¦å¯è§†åŒ–ï¼‰")
    print(f"{'='*70}\n")
    
    # é¢„å…ˆåŠ è½½æ•°æ®
    load_data_once()
    G, _, _, _, _, _ = get_data()
    
    start_time = time_module.time()
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    results_all = {}
    
    try:
        # è¿è¡Œæµ‹è¯•1
        print("è¿è¡Œæµ‹è¯•1...")
        results_all['test1'] = test_1_basic_solve()
        
        # è¿è¡Œæµ‹è¯•2ï¼ˆå¢å¼ºç‰ˆï¼‰
        print("è¿è¡Œæµ‹è¯•2...")
        results_all['test2'] = test_2_alpha_sensitivity()
        
        # è¿è¡Œæµ‹è¯•3
        print("è¿è¡Œæµ‹è¯•3...")
        results_all['test3'] = []  # å¯é€‰
        
        # è¿è¡Œæµ‹è¯•5ï¼ˆä¿®æ”¹ç‰ˆï¼‰
        print("è¿è¡Œæµ‹è¯•5...")
        results_all['test5'] = test_5_multiple_od_pairs()
        
        total_time = time_module.time() - start_time
        
        print(f"\næ‰€æœ‰æµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        # âœ… ä¿å­˜ç»“æœ
        print("\nä¿å­˜æµ‹è¯•ç»“æœ...")
        save_results(results_all, solver_type='reverse', output_dir=f'results/{testname}')


        
        # âœ… ç”Ÿæˆå¯è§†åŒ–
        print("\nç”ŸæˆHTML+SVGå¯è§†åŒ–...")
        from visualization_generator import generate_html_from_files
        generate_html_from_files(
            G=G,
            reverse_file='results/reverse_results_latest.json',
            forward_file=None,  # å¦‚æœæœ‰æ­£å‘ç»“æœå¯ä»¥ä¼ å…¥
            output_file='solver_visualization.html'
        )
        
        return True
        
    except Exception as e:
        print(f"\næµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»ç¨‹åº
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½å’Œé¢„è®¡ç®—
    print("\næµ‹è¯•æ•°æ®åŠ è½½å’Œé¢„è®¡ç®—...")
    
    load_data_once()
    
    # è·å–åŸºç¡€æ•°æ®
    G, sparse_data, node_to_index, scenario_dates, scenario_probs, time_intervals_per_day = get_data()
    
    # âœ¨ è·å–é¢„è®¡ç®—æ•°æ®
    adj_list_forward, adj_list_backward, link_distributions = get_precomputed_data()
    
    print("\næ•°æ®ç»Ÿè®¡:")
    print(f"  èŠ‚ç‚¹æ•°: {len(G.nodes())}")
    print(f"  è¾¹æ•°: {len(G.edges())}")
    print(f"  æ­£å‘é‚»æ¥è¡¨èŠ‚ç‚¹æ•°: {len(adj_list_forward)}")
    print(f"  åå‘é‚»æ¥è¡¨èŠ‚ç‚¹æ•°: {len(adj_list_backward)}")
    print(f"  é“¾è·¯åˆ†å¸ƒæ•°: {len(link_distributions)}")
    
    # æµ‹è¯•é‚»æ¥è¡¨
    sample_node = list(adj_list_forward.keys())[0]
    print(f"\nç¤ºä¾‹èŠ‚ç‚¹ {sample_node} çš„é‚»æ¥èŠ‚ç‚¹:")
    print(f"  æ­£å‘é‚»æ¥:  {adj_list_forward.get(sample_node, [])[: 5]} ...")
    if sample_node in adj_list_backward:
        print(f"  åå‘é‚»æ¥: {adj_list_backward[sample_node][:5]} ...")
    
    print("\nâœ“ æ•°æ®åŠ è½½å’Œé¢„è®¡ç®—æµ‹è¯•å®Œæˆï¼")