"""
åå‘Label-Settingæ±‚è§£å™¨ - ä¸¥æ ¼æŒ‰ç…§ä¼ªä»£ç å®ç°ï¼ˆå®Œæ•´ç‰ˆï¼‰
åŒ…å«è¯¦ç»†çš„æ¦‚ç‡è®¡ç®—è¯´æ˜
"""

import numpy as np
import heapq
import time
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass, field


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®ç»“æ„å®šä¹‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class AlphaDiscreteDistribution:
    """Î±-ç¦»æ•£åˆ†å¸ƒç±»"""
    values: np.ndarray
    L1:  int
    weights: Optional[np.ndarray] = None
    
    def __post_init__(self):
        """ååˆå§‹åŒ–"""
        if not isinstance(self.values, np.ndarray):
            self.values = np.array(self.values)
        
        if self.weights is None:
            self.weights = np.ones(len(self.values)) / len(self.values)
        elif not isinstance(self.weights, np.ndarray):
            self.weights = np.array(self.weights)
        
        if len(self.weights) > 0 and self.weights.sum() > 0:
            self.weights = self.weights / self.weights.sum()
    
    def get_quantile(self, alpha: float) -> float:
        """è®¡ç®—Î±åˆ†ä½æ•°"""
        if len(self.values) == 0:
            return float('inf')
        if alpha <= 0:
            return float(self.values.min())
        if alpha >= 1:
            return float(self.values.max())
        
        sorted_idx = np.argsort(self.values)
        sorted_vals = self.values[sorted_idx]
        sorted_wts = self.weights[sorted_idx]
        cumsum = np.cumsum(sorted_wts)
        
        return float(np.interp(alpha, cumsum, sorted_vals))
    
    def get_mean(self) -> float:
        """è®¡ç®—å‡å€¼"""
        return float(np.sum(self.values * self.weights))
    
    def get_expected(self) -> float:
        """è®¡ç®—æœŸæœ›å€¼ï¼ˆä¸ get_mean ç›¸åŒï¼‰"""
        return self.get_mean()
    
    def get_variance(self) -> float:
        """è®¡ç®—æ–¹å·®"""
        mu = self.get_mean()
        return float(np.sum(self.weights * (self.values - mu) ** 2))
    
    def get_std(self) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        return float(np.sqrt(self.get_variance()))
    
    def get_median(self) -> float:
        return np.median(self.values)
    
    
    def reverse_convolve(self,
                    get_link_dist_func,
                    predecessor:  int,
                    current:  int,
                    time_intervals_per_day: int,
                    L2:  int,
                    verbose: bool = False) -> 'AlphaDiscreteDistribution':
        """
        åå‘å·ç§¯ï¼ˆåŸºäºç²¾ç¡®æ¦‚ç‡è®¡ç®—ï¼‰
        
        Args:
            get_link_dist_func: è·å–é“¾è·¯åˆ†å¸ƒçš„å‡½æ•°
            predecessor: å‰é©±èŠ‚ç‚¹u
            current: å½“å‰èŠ‚ç‚¹v
            time_intervals_per_day: æ¯å¤©æ—¶é—´ç‰‡æ•°
            L2: æœªä½¿ç”¨ï¼ˆä¿æŒæ¥å£å…¼å®¹ï¼‰
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            å‡ºå‘æ—¶é—´åˆ†å¸ƒ A(u)
        """
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤1: è·å–å¯ç”¨æ—¶é—´ç‰‡å’Œæ—…è¡Œæ—¶é—´èŒƒå›´
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        available_slots = self._get_available_slots(get_link_dist_func, predecessor, current)
        
        if not available_slots: 
            raise ValueError(f"è¾¹({predecessor}, {current})æ²¡æœ‰é“¾è·¯åˆ†å¸ƒæ•°æ®")
        
        # ä¼°ç®—æ—…è¡Œæ—¶é—´èŒƒå›´
        min_travel = float('inf')
        max_travel = 0
        
        for slot in available_slots: 
            D_slot = get_link_dist_func(predecessor, current, slot)
            if D_slot and D_slot.times:
                min_travel = min(min_travel, min(D_slot.times))
                max_travel = max(max_travel, max(D_slot.times))
        
        if min_travel == float('inf'):
            raise ValueError(f"æ— æ³•è·å–è¾¹({predecessor}, {current})çš„æ—…è¡Œæ—¶é—´èŒƒå›´")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤2: ç¡®å®šå‡ºå‘æ—¶é—´æœç´¢èŒƒå›´
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        min_arrival = float(np.min(self.values))
        max_arrival = float(np.max(self.values))
        
        min_departure = min_arrival - max_travel
        max_departure = max_arrival - min_travel
        
        # ç¦»æ•£åŒ–æ­¥é•¿ï¼ˆ1åˆ†é’Ÿ = 10ä¸ªå•ä½ï¼‰
        step = 1
        
        # ç”Ÿæˆå€™é€‰å‡ºå‘æ—¶é—´
        candidate_departures = np.arange(
            int(np.floor(min_departure)),
            int(np.ceil(max_departure)) + step,
            step
        )
        
        if verbose:
            print(f"    å‡ºå‘æ—¶é—´èŒƒå›´: [{min_departure:.1f}, {max_departure:.1f}]")
            print(f"    å€™é€‰å‡ºå‘æ—¶é—´æ•°: {len(candidate_departures)}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤3: æ„å»ºåˆ°è¾¾æ—¶é—´çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆâœ… ä½¿ç”¨å®é™…æƒé‡ï¼‰
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        arrival_probs = {}
        
        for i, t_arr in enumerate(self.values):
            # âœ… ä½¿ç”¨å®é™…æƒé‡è€Œä¸æ˜¯å‡è®¾ç­‰æƒé‡
            weight = self.weights[i] if self.weights is not None else 1.0 / self.L1
            
            t_arr_int = int(round(t_arr))
            if t_arr_int not in arrival_probs:
                arrival_probs[t_arr_int] = 0.0
            arrival_probs[t_arr_int] += weight
        
        # å½’ä¸€åŒ–åˆ°è¾¾æ—¶é—´æ¦‚ç‡
        total_arrival_prob = sum(arrival_probs.values())
        if total_arrival_prob > 0:
            for t in arrival_probs:
                arrival_probs[t] /= total_arrival_prob
        
        if verbose:
            print(f"    åˆ°è¾¾æ—¶é—´åˆ†å¸ƒå¤§å°: {len(arrival_probs)}")
            print(f"    åˆ°è¾¾æ—¶é—´æ¦‚ç‡å’Œ: {sum(arrival_probs.values()):.6f}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤4: å¯¹æ¯ä¸ªå€™é€‰å‡ºå‘æ—¶é—´ï¼Œè®¡ç®—å…¶æ¦‚ç‡
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        departure_probs = {}
        
        for t_dep in candidate_departures:
            # ç¡®å®šå‡ºå‘æ—¶é—´å¯¹åº”çš„æ—¶é—´ç‰‡
            slot_dep = int(t_dep / 10) % time_intervals_per_day
            
            # è·å–è¯¥æ—¶é—´ç‰‡çš„è·¯æ®µåˆ†å¸ƒ
            D_slot = get_link_dist_func(predecessor, current, slot_dep)
            
            if D_slot is None:
                # å°è¯•æœ€è¿‘çš„æ—¶é—´ç‰‡
                nearest_slot = self._find_nearest_slot(
                    slot_dep, available_slots, time_intervals_per_day
                )
                D_slot = get_link_dist_func(predecessor, current, nearest_slot)
            
            if D_slot is None:
                continue
            
            # è®¡ç®— P(t_dep)
            prob_t_dep = 0.0
            
            for t_arr, prob_arr in arrival_probs.items():
                # è®¡ç®—æ‰€éœ€æ—…è¡Œæ—¶é—´
                required_travel_time = t_arr - t_dep
                
                # æŸ¥è¯¢è¯¥æ—…è¡Œæ—¶é—´çš„æ¦‚ç‡
                prob_travel = D_slot.get_probability(required_travel_time)
                
                if prob_travel > 0:
                    prob_t_dep += prob_arr * prob_travel
            
            if prob_t_dep > 1e-10:  # ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤æå°æ¦‚ç‡
                departure_probs[t_dep] = prob_t_dep
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤5: å½’ä¸€åŒ–å¹¶æ„é€ æ–°åˆ†å¸ƒ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        if not departure_probs:
            if verbose:
                print(f"    è­¦å‘Š:  æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„å‡ºå‘æ¦‚ç‡")
                print(f"    å€™é€‰æ•°: {len(candidate_departures)}, åˆ°è¾¾æ•°: {len(arrival_probs)}")
            
            # è¿”å›ä¸€ä¸ªåŸºäºæœ€æ—©å‡ºå‘æ—¶é—´çš„é»˜è®¤åˆ†å¸ƒ
            default_time = min_departure
            return AlphaDiscreteDistribution(
                values=np.array([default_time]),
                L1=1,
                weights=np.array([1.0])
            )
        
        # è®¡ç®—æ€»æ¦‚ç‡
        total_prob = sum(departure_probs.values())
        
        if total_prob <= 1e-10:
            if verbose:
                print(f"    è­¦å‘Š: æ€»æ¦‚ç‡è¿‡å°:  {total_prob:.10e}")
            
            # ä½¿ç”¨æœªå½’ä¸€åŒ–çš„åˆ†å¸ƒ
            times = np.array(sorted(departure_probs.keys()))
            weights = np.ones(len(times)) / len(times)
            return AlphaDiscreteDistribution(
                values=times[: self.L1] if len(times) > self.L1 else times,
                L1=min(len(times), self.L1),
                weights=weights[: self.L1] if len(weights) > self.L1 else weights
            )
        
        # âœ… ä¸€æ¬¡æ€§å½’ä¸€åŒ–ï¼ˆé¿å…é‡å¤ï¼‰
        for t in departure_probs:
            departure_probs[t] /= total_prob
        
        # è½¬æ¢ä¸ºæ•°ç»„
        times = np.array(sorted(departure_probs.keys()), dtype=float)
        probs = np.array([departure_probs[t] for t in times], dtype=float)
        
        # éªŒè¯æ¦‚ç‡å’Œ
        prob_sum = probs.sum()
        if abs(prob_sum - 1.0) > 1e-6:
            if verbose:
                print(f"    è­¦å‘Š:  æ¦‚ç‡å’Œ = {prob_sum:.6f} != 1.0ï¼Œé‡æ–°å½’ä¸€åŒ–")
            probs = probs / prob_sum
        
        if verbose:
            print(f"    å‡ºå‘æ—¶é—´åˆ†å¸ƒå¤§å°:   {len(times)}")
            print(f"    æ¦‚ç‡å’Œ:  {probs.sum():.6f}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ç¡®å®šæ€§åˆ†ä½æ•°é‡‡æ ·ï¼ˆä¿ç•™åˆ†å¸ƒå½¢çŠ¶ï¼‰
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        if len(times) > self.L1:
            # éœ€è¦é™é‡‡æ ·
            cdf = np.cumsum(probs)
            target_quantiles = np.linspace(1/(self.L1 + 1), self.L1/(self.L1 + 1), self.L1)
            sampled_times = np.interp(target_quantiles, cdf, times)
            sampled_weights = np.ones(self.L1) / self.L1
            
            if verbose:
                print(f"    é™é‡‡æ ·:  {len(times)} -> {self.L1} ä¸ªç‚¹")
        
        elif len(times) < self.L1:
            # éœ€è¦ä¸Šé‡‡æ ·
            cdf = np.cumsum(probs)
            target_quantiles = np.linspace(1/(self.L1 + 1), self.L1/(self.L1 + 1), self.L1)
            sampled_times = np.interp(target_quantiles, cdf, times)
            sampled_weights = np.ones(self.L1) / self.L1
            
            if verbose:
                print(f"    ä¸Šé‡‡æ ·: {len(times)} -> {self.L1} ä¸ªç‚¹")
        
        else:
            # å¤§å°åˆšå¥½
            sampled_times = times
            sampled_weights = probs  # âœ… ä½¿ç”¨å®é™…æ¦‚ç‡è€Œä¸æ˜¯å‡åŒ€æƒé‡
            
            if verbose:
                print(f"    ä¿æŒåŸå§‹å¤§å°: {self.L1} ä¸ªç‚¹")
        
        # ç¡®ä¿æœ‰åº
        sorted_indices = np.argsort(sampled_times)
        sampled_times = sampled_times[sorted_indices]
        sampled_weights = sampled_weights[sorted_indices]
        
        # æœ€ç»ˆå½’ä¸€åŒ–ï¼ˆç¡®ä¿æƒé‡å’Œä¸º1ï¼‰
        sampled_weights = sampled_weights / sampled_weights.sum()
        
        if verbose: 
            print(f"    æœ€ç»ˆåˆ†å¸ƒ:")
            print(f"      å¤§å°: {len(sampled_times)}")
            print(f"      æ—¶é—´èŒƒå›´: [{sampled_times[0]:.1f}, {sampled_times[-1]:.1f}]")
            print(f"      æƒé‡å’Œ: {sampled_weights.sum():.6f}")
            print(f"      æœŸæœ›å€¼: {np.sum(sampled_times * sampled_weights):.2f}")
        
        return AlphaDiscreteDistribution(
            values=sampled_times,
            L1=self.L1,
            weights=sampled_weights
        )



    def _find_nearest_slot(self, target_slot: int, available_slots: List[int],
                        time_intervals_per_day: int) -> int:
        """æ‰¾åˆ°æœ€è¿‘çš„æ—¶é—´ç‰‡"""
        min_dist = float('inf')
        best_slot = available_slots[0]
        
        for slot in available_slots:
            dist = abs(slot - target_slot)
            cyclic_dist = min(dist, time_intervals_per_day - dist)
            
            if cyclic_dist < min_dist:
                min_dist = cyclic_dist
                best_slot = slot
        
        return best_slot
    
    def _get_available_slots(self, get_link_dist_func, u: int, v: int) -> List[int]:
        if not hasattr(AlphaDiscreteDistribution, '_slot_cache'):
            AlphaDiscreteDistribution._slot_cache = {}
        cache_key = (u, v)
        if cache_key in AlphaDiscreteDistribution._slot_cache:
            return AlphaDiscreteDistribution._slot_cache[cache_key]
        available = []
        try:
            link_distributions = get_link_dist_func.__self__.link_distributions
            for (link_u, link_v, slot) in link_distributions.keys():
                if link_u == u and link_v == v:
                    available.append(slot)
        except AttributeError:
            raise ValueError("æ— æ³•è®¿é—®é“¾è·¯åˆ†å¸ƒæ•°æ®")
        result = sorted(set(available))
        AlphaDiscreteDistribution._slot_cache[cache_key] = result
        return result
    
    def _get_slots_in_range(self, slot_min: int, slot_max: int,
                           available_slots: List[int],
                           time_intervals_per_day: int) -> List[int]:
        """è·å–èŒƒå›´[slot_min, slot_max]å†…çš„å€™é€‰æ—¶é—´ç‰‡"""
        candidate_slots = []
        
        if slot_min <= slot_max:
            # æ­£å¸¸èŒƒå›´
            for slot in available_slots:
                if slot_min <= slot <= slot_max:
                    candidate_slots.append(slot)
        else:
            # è·¨å¤©èŒƒå›´
            for slot in available_slots:
                if slot >= slot_min or slot <= slot_max:
                    candidate_slots.append(slot)
        
        return sorted(candidate_slots)


@dataclass
class LinkTimeDistribution:
    """è·¯æ®µæ—…è¡Œæ—¶é—´åˆ†å¸ƒ"""
    time_prob: Dict[int, float]
    times: List[int]
    cdf: List[float]
    time_slot: int
    
    def __init__(self, time_prob_dict: Dict[int, float], time_slot: int = None):
        if not time_prob_dict:
            raise ValueError("é“¾è·¯åˆ†å¸ƒä¸èƒ½ä¸ºç©º")
        
        total_prob = sum(time_prob_dict.values())
        self.time_prob = {t: p/total_prob for t, p in time_prob_dict.items()}
        self.time_slot = time_slot
        
        sorted_times = sorted(self.time_prob.keys())
        self.times = sorted_times
        
        cumulative = 0.0
        self.cdf = []
        for t in sorted_times:
            cumulative += self.time_prob[t]
            self.cdf.append(cumulative)
    
    def sample_L2_times(self, reference_time: int, L2: int) -> List[int]:
        """é‡‡æ ·L2ä¸ªæ—…è¡Œæ—¶é—´ï¼ˆé€†CDFæ–¹æ³•ï¼‰"""
        samples = []
        for i in range(1, L2 + 1):
            quantile = i / (L2 + 1)
            sample = self._inverse_cdf(quantile)
            samples.append(sample)
        return sorted(samples)
    
    def _inverse_cdf(self, quantile: float) -> int:
        """é€†CDFï¼ˆçº¿æ€§æ’å€¼ï¼‰"""
        if quantile <= 0:
            return self.times[0]
        if quantile >= 1:
            return self.times[-1]
        
        for i, cdf_val in enumerate(self.cdf):
            if cdf_val >= quantile:
                if i == 0:
                    return self.times[0]
                
                lower_cdf = self.cdf[i-1] if i > 0 else 0
                upper_cdf = cdf_val
                lower_time = self.times[i-1] if i > 0 else self.times[0]
                upper_time = self.times[i]
                
                if upper_cdf > lower_cdf:
                    weight = (quantile - lower_cdf) / (upper_cdf - lower_cdf)
                else:
                    weight = 0.5
                
                return int(round(lower_time + weight * (upper_time - lower_time)))
        
        return self.times[-1]
    
    def get_probability(self, travel_time: float) -> float:
        """
        è·å–æŒ‡å®šæ—…è¡Œæ—¶é—´çš„æ¦‚ç‡
        
        Args:
            travel_time: æ—…è¡Œæ—¶é—´ï¼ˆ0.1åˆ†é’Ÿå•ä½ï¼‰
            
        Returns:
            è¯¥æ—…è¡Œæ—¶é—´çš„æ¦‚ç‡ï¼ˆå¦‚æœä¸åœ¨æ”¯æŒé›†ä¸­ï¼Œå¯é€‰æ‹©æ’å€¼æˆ–è¿”å›0ï¼‰
        """
        # è½¬æ¢ä¸ºæ•´æ•°ï¼ˆä¸å­˜å‚¨çš„é”®åŒ¹é…ï¼‰
        travel_time_int = int(round(travel_time))
        
        # ç²¾ç¡®åŒ¹é…
        if travel_time_int in self.time_prob:
            return self.time_prob[travel_time_int]
        
        # å¯é€‰ï¼šçº¿æ€§æ’å€¼ï¼ˆå¦‚æœéœ€è¦æ›´å¹³æ»‘çš„ç»“æœï¼‰
        if self.times:
            min_time = self.times[0]
            max_time = self.times[-1]
            
            if travel_time_int < min_time or travel_time_int > max_time:
                return 0.0
            
            # æ‰¾åˆ°ç›¸é‚»çš„ä¸¤ä¸ªç‚¹
            for i in range(len(self.times) - 1):
                if self.times[i] <= travel_time_int <= self.times[i+1]:
                    # çº¿æ€§æ’å€¼
                    t_lower = self.times[i]
                    t_upper = self.times[i+1]
                    p_lower = self.time_prob[t_lower]
                    p_upper = self.time_prob[t_upper]
                    
                    if t_upper == t_lower:
                        return p_lower
                    
                    weight = (travel_time_int - t_lower) / (t_upper - t_lower)
                    return p_lower * (1 - weight) + p_upper * weight
        
        return 0.0

    def get_mean(self) -> float:
        return sum(t * p for t, p in self.time_prob.items())
    
    def get_std(self) -> float:
        mean = self.get_mean()
        variance = sum(p * (t - mean)**2 for t, p in self.time_prob.items())
        return np.sqrt(variance)


@dataclass
class ReverseLabel:
    """åå‘æœç´¢æ ‡ç­¾"""
    node_id: int
    distribution: 'AlphaDiscreteDistribution'
    path: List[int]
    cost: float
    quantile_cache: Dict[float, float] = field(default_factory=dict)
    
    # âœ… æ·»åŠ ç¼“å­˜å±æ€§ï¼ˆåœ¨ __post_init__ ä¸­è®¾ç½®ï¼‰
    mean_cache: float = field(default=0.0, init=False, repr=False)
    variance_cache: float = field(default=0.0, init=False, repr=False)
    std_cache: float = field(default=0.0, init=False, repr=False)
    
    def __post_init__(self):
        """ååˆå§‹åŒ–ï¼šé¢„è®¡ç®—å¸¸ç”¨ç»Ÿè®¡é‡"""
        # é¢„è®¡ç®—å¸¸ç”¨åˆ†ä½æ•°
        for q in [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:
            self.quantile_cache[q] = self.distribution.get_quantile(q)
        
        # é¢„è®¡ç®—å‡å€¼ã€æ–¹å·®å’Œæ ‡å‡†å·®
        self.mean_cache = self.distribution.get_mean()
        self.variance_cache = self.distribution.get_variance()
        self.std_cache = self.distribution.get_std()
    
    def __lt__(self, other:  'ReverseLabel') -> bool:
        """
        ç”¨äºä¼˜å…ˆé˜Ÿåˆ—æ’åº
        åå‘æ±‚è§£ï¼šcost è¶Šå¤§è¶Šä¼˜å…ˆï¼ˆæœ€æ™šå‡ºå‘æ—¶é—´ï¼‰
        """
        return self.cost > other.cost
    
    @property
    def expected_value(self) -> float:
        """æœŸæœ›å€¼ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        return self.mean_cache
    
    @property
    def std_value(self) -> float:
        """æ ‡å‡†å·®ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        return self.std_cache
    
    @property
    def variance_value(self) -> float:
        """æ–¹å·®ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        return self.variance_cache
    
    def get_cached_quantile(self, alpha:  float) -> float:
        """
        è·å–ç¼“å­˜çš„åˆ†ä½æ•°
        
        Args: 
            alpha: åˆ†ä½æ•° (0-1)
            
        Returns: 
            float: åˆ†ä½æ•°å€¼
        """
        if alpha in self.quantile_cache:
            return self.quantile_cache[alpha]
        
        # è®¡ç®—å¹¶ç¼“å­˜
        value = self.distribution.get_quantile(alpha)
        self.quantile_cache[alpha] = value
        return value
    
    def get_quantile(self, alpha: float) -> float:
        """
        è·å–åˆ†ä½æ•°ï¼ˆå¤–éƒ¨æ¥å£ï¼‰
        
        Args:
            alpha: åˆ†ä½æ•° (0-1)
            
        Returns:
            float: åˆ†ä½æ•°å€¼
        """
        return self.get_cached_quantile(alpha)
    
    def dominates_weak(self, other:  'ReverseLabel', alpha: float, epsilon: float = 1e-6) -> bool:
        """
        ä¸¥æ ¼çš„å¤šç›®æ ‡æ”¯é…è§„åˆ™ï¼ˆMOSPï¼‰
        
        åœ¨åå‘æ±‚è§£ä¸­ï¼Œä¼˜åŒ–ç›®æ ‡ä¸ºï¼š
        1.ä¸»ç›®æ ‡ï¼šmax Q_Î± (Î±åˆ†ä½æ•°ï¼Œæœ€æ™šå‡ºå‘æ—¶é—´)
        2.æ¬¡è¦ç›®æ ‡1ï¼šmax Î¼ (æœŸæœ›å‡ºå‘æ—¶é—´)
        3.æ¬¡è¦ç›®æ ‡2ï¼šmin ÏƒÂ² (æ–¹å·®ï¼Œç¨³å®šæ€§)
        
        æ”¯é…æ¡ä»¶ï¼š
        self ä¸¥æ ¼æ”¯é… otherï¼Œå½“ä¸”ä»…å½“ï¼š
        - Q_Î±(self) > Q_Î±(other)  (ä¸»ç›®æ ‡ä¸¥æ ¼æ›´ä¼˜)
        - Î¼(self) >= Î¼(other)      (æ¬¡è¦ç›®æ ‡1ä¸åŠ£)
        - ÏƒÂ²(self) <= ÏƒÂ²(other)    (æ¬¡è¦ç›®æ ‡2ä¸åŠ£)
        
        æˆ–è€…ï¼š
        - Q_Î±(self) >= Q_Î±(other)  (ä¸»ç›®æ ‡ä¸åŠ£)
        - è‡³å°‘ä¸€ä¸ªæ¬¡è¦ç›®æ ‡ä¸¥æ ¼æ›´ä¼˜
        - å…¶ä»–æ¬¡è¦ç›®æ ‡ä¸åŠ£
        
        Args:
            other:  å¦ä¸€ä¸ªæ ‡ç­¾
            alpha: å¯é æ€§å‚æ•°
            epsilon: æ•°å€¼å®¹å·®
            
        Returns:
            bool: æ˜¯å¦æ”¯é…
        """
        if self.node_id != other.node_id:
            return False
        
        # è·å–ä¸»ç›®æ ‡ï¼šÎ±åˆ†ä½æ•°ï¼ˆæœ€æ™šå‡ºå‘æ—¶é—´ï¼‰
        # æ³¨æ„ï¼šåå‘CDFä¸­ï¼Œæˆ‘ä»¬è¦æ‰¾çš„æ˜¯ 1-Î± åˆ†ä½æ•°
        Q_self = self.get_cached_quantile(1 - alpha)
        Q_other = other.get_cached_quantile(1 - alpha)
        
        # è·å–æ¬¡è¦ç›®æ ‡1ï¼šæœŸæœ›å€¼
        mu_self = self.expected_value
        mu_other = other.expected_value
        
        # è·å–æ¬¡è¦ç›®æ ‡2ï¼šæ–¹å·®
        sigma2_self = self.variance_value
        sigma2_other = other.variance_value
        
        # ç­–ç•¥1ï¼šä¸»ç›®æ ‡ä¸¥æ ¼æ›´ä¼˜ + æ¬¡è¦ç›®æ ‡ä¸åŠ£
        if Q_self > Q_other + epsilon:   # ä¸»ç›®æ ‡ä¸¥æ ¼æ›´ä¼˜ï¼ˆæ›´æ™šå‡ºå‘ï¼‰
            # æ£€æŸ¥æ¬¡è¦ç›®æ ‡æ˜¯å¦ä¸åŠ£
            mu_ok = mu_self >= mu_other - epsilon       # æœŸæœ›ä¸åŠ£
            sigma_ok = sigma2_self <= sigma2_other + epsilon  # æ–¹å·®ä¸åŠ£
            
            if mu_ok and sigma_ok: 
                return True
        
        # ç­–ç•¥2ï¼šä¸»ç›®æ ‡ä¸åŠ£ + è‡³å°‘ä¸€ä¸ªæ¬¡è¦ç›®æ ‡ä¸¥æ ¼æ›´ä¼˜
        if Q_self >= Q_other - epsilon:  # ä¸»ç›®æ ‡ä¸åŠ£
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¬¡è¦ç›®æ ‡ä¸¥æ ¼æ›´ä¼˜
            mu_better = mu_self > mu_other + epsilon
            sigma_better = sigma2_self < sigma2_other - epsilon
            
            # è‡³å°‘ä¸€ä¸ªæ¬¡è¦ç›®æ ‡ä¸¥æ ¼æ›´ä¼˜
            if mu_better or sigma_better:
                # æ£€æŸ¥å…¶ä»–ç›®æ ‡æ˜¯å¦ä¸åŠ£
                mu_ok = mu_self >= mu_other - epsilon
                sigma_ok = sigma2_self <= sigma2_other + epsilon
                
                if mu_ok and sigma_ok:
                    return True
        
        return False
    
    def dominates(self, other:  'ReverseLabel', alpha: float, epsilon: float = 1e-6) -> bool:
        """
        ç»Ÿä¸€çš„æ”¯é…æ£€æŸ¥æ¥å£
        
        Args:
            other: å¦ä¸€ä¸ªæ ‡ç­¾
            alpha:  å¯é æ€§å‚æ•°
            epsilon: æ•°å€¼å®¹å·®
            
        Returns:
            bool: æ˜¯å¦æ”¯é…
        """
        return self.dominates_weak(other, alpha, epsilon)
    
    def __eq__(self, other: object) -> bool:
        """ç›¸ç­‰æ€§åˆ¤æ–­"""
        if not isinstance(other, ReverseLabel):
            return False
        return (self.node_id == other.node_id and 
                abs(self.cost - other.cost) < 1e-9)
    
    def __hash__(self) -> int:
        """å“ˆå¸Œå€¼ï¼ˆç”¨äºé›†åˆå’Œå­—å…¸ï¼‰"""
        return hash((self.node_id, round(self.cost, 6)))
    
    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"ReverseLabel(node={self.node_id}, "
                f"cost={self.cost:.2f}, "
                f"E={self.expected_value:.2f}, "
                f"Ïƒ={self.std_value:.2f}, "
                f"path_len={len(self.path)})")
    
    def copy(self) -> 'ReverseLabel':
        """
        åˆ›å»ºæ ‡ç­¾çš„å‰¯æœ¬
        
        Returns:
            ReverseLabel: æ–°çš„æ ‡ç­¾å‰¯æœ¬
        """
        return ReverseLabel(
            node_id=self.node_id,
            distribution=self.distribution,  # åˆ†å¸ƒå¯¹è±¡å¯ä»¥å…±äº«
            path=self.path.copy(),  # è·¯å¾„éœ€è¦å¤åˆ¶
            cost=self.cost
        )
    
    def get_statistics(self) -> dict:
        """
        è·å–æ ‡ç­¾çš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            dict:  ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            'node_id': self.node_id,
            'cost':  self.cost,
            'expected':  self.expected_value,
            'std': self.std_value,
            'variance': self.variance_value,
            'median': self.get_cached_quantile(0.5),
            'q05': self.get_cached_quantile(0.05),
            'q95': self.get_cached_quantile(0.95),
            'path_length': len(self.path),
            'path': self.path
        }



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åå‘æ±‚è§£å™¨ä¸»ç±»
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ReverseLabelSettingSolver:
    """åå‘Label-Settingæ±‚è§£å™¨ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
    
    def __init__(self, G, sparse_data, node_to_index, scenario_dates,
                 scenario_probs, time_intervals_per_day,
                 L1: int = 50, L2: int = 10,K = 10,
                 verbose: bool = False,
                 max_labels_per_node: int = 20,
             adj_list=None,  # âœ¨ æ–°å¢
             reverse_adj_list=None,
             link_distributions=None):  # âœ¨ æ–°å¢
        """åˆå§‹åŒ–"""
        self.G = G
        self.sparse_data = sparse_data
        self.node_to_index = node_to_index
        self.index_to_node = {v: k for k, v in node_to_index.items()}
        self.scenario_dates = scenario_dates
        self.scenario_probs = scenario_probs
        self.time_intervals_per_day = time_intervals_per_day
        self.n_scenarios = len(scenario_dates)
        
        self.L1 = L1
        self.L2 = L2
        self.verbose = verbose

        self.max_labels_per_node = max_labels_per_node
        
        print(f"\n{'='*70}")
        print(f"åˆå§‹åŒ–åå‘Label-Settingæ±‚è§£å™¨ï¼ˆå®Œæ•´ç‰ˆï¼‰")
        print(f"{'='*70}")
        print(f"  ç®—æ³•: åå‘Label-Setting with æ¦‚ç‡æƒé‡")
        print(f"  é—®é¢˜: é¢„ç•™æ—¶é—´é¢„ç®—")
        print(f"  å‚æ•°: L1={L1}, L2={L2}")
        print(f"  è¯¦ç»†è¾“å‡º: {'å¼€å¯' if verbose else 'å…³é—­'}")
        
        # æ„å»ºé‚»æ¥è¡¨
        self.adj_list = defaultdict(list)
        self.reverse_adj_list = defaultdict(list) 
        # é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ
        self.link_distributions = {}

        # å¦‚æœä¼ å…¥äº†é¢„è®¡ç®—æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
        if adj_list is not None and reverse_adj_list is not None:
            self.adj_list = adj_list
            self.reverse_adj_list=reverse_adj_list
            print("  âœ“ ä½¿ç”¨é¢„è®¡ç®—é‚»æ¥è¡¨")
        else:
            # å¦åˆ™è‡ªå·±æ„å»º
            self._build_adjacency_lists()
        
        if link_distributions is not None: 
            self.link_distributions = link_distributions
            print("  âœ“ ä½¿ç”¨é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ")
        else:
            # å¦åˆ™è‡ªå·±è®¡ç®—
            self._precompute_link_distributions()

        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = defaultdict(int)
        self.origin_labels_history = []
        
        print(f"\nâœ“ åˆå§‹åŒ–å®Œæˆ")
        print(f"{'='*70}\n")
    
    def _build_adjacency_lists(self):
        """æ„å»ºé‚»æ¥è¡¨"""
        print(f"  [1/2] æ„å»ºé‚»æ¥è¡¨...")
        start_time = time.time()
        
        edges_set = set()
        for (scenario_idx, time_idx, from_idx, to_idx) in self.sparse_data.keys():
            if scenario_idx < self.n_scenarios:
                from_node = self.index_to_node[from_idx]
                to_node = self.index_to_node[to_idx]
                edges_set.add((from_node, to_node))
        
        for from_node, to_node in edges_set:
            self.adj_list[from_node].append(to_node)
            self.reverse_adj_list[to_node].append(from_node)
        
        elapsed = time.time() - start_time
        print(f"      âœ“ å®Œæˆ (ç”¨æ—¶ {elapsed:.2f}s) - {len(edges_set):,} æ¡è¾¹")
    
    def _precompute_link_distributions(self):
        """é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ"""
        print(f"  [2/2] é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ...")
        start_time = time.time()
        
        link_time_data = defaultdict(list)
        
        for (scenario_idx, time_idx, from_idx, to_idx), travel_time_minutes in self.sparse_data.items():
            if scenario_idx >= self.n_scenarios:
                continue
            
            from_node = self.index_to_node[from_idx]
            to_node = self.index_to_node[to_idx]
            travel_time_01min = int(travel_time_minutes * 10)
            
            link_time_data[(from_node, to_node, time_idx)].append(travel_time_01min)
        
        distribution_count = 0
        for (u, v, t), times in link_time_data.items():
            time_counts = defaultdict(int)
            for time_val in times:
                time_counts[time_val] += 1
            
            total = len(times)
            time_prob = {time_val: count/total for time_val, count in time_counts.items()}
            
            try:
                self.link_distributions[(u, v, t)] = LinkTimeDistribution(time_prob, time_slot=t)
                distribution_count += 1
            except ValueError:
                continue
        
        elapsed = time.time() - start_time
        print(f"      âœ“ å®Œæˆ (ç”¨æ—¶ {elapsed:.2f}s) - {distribution_count:,} ä¸ªåˆ†å¸ƒ")
    
    def _get_link_distribution_at_slot(self, u: int, v: int, slot: int) -> Optional[LinkTimeDistribution]:
        """è·å–æŒ‡å®šå‡ºå‘æ—¶é—´ç‰‡çš„é“¾è·¯åˆ†å¸ƒ"""
        if (u, v, slot) in self.link_distributions:
            return self.link_distributions[(u, v, slot)]
        
        # å®¹å·®åŒ¹é…
        tolerance = 5
        candidates = []
        
        for (link_u, link_v, link_t) in self.link_distributions.keys():
            if link_u == u and link_v == v:
                diff = abs(link_t - slot)
                cyclic_diff = min(diff, self.time_intervals_per_day - diff)
                
                if cyclic_diff <= tolerance:
                    candidates.append((link_t, cyclic_diff))
        
        if candidates:
            best_slot = min(candidates, key=lambda x: x[1])[0]
            return self.link_distributions[(u, v, best_slot)]
        
        return None
    



    def solve_k_paths(self, origin: int, destination: int, target_arrival_time: int,
                     alpha: float, K: int = 10, max_labels:  int = 100000,
                     print_interval: int = 100) -> Dict: 
        """
        K-Paths åå‘æ±‚è§£
        
        Args: 
            origin: èµ·ç‚¹
            destination: ç»ˆç‚¹
            target_arrival_time: ç›®æ ‡åˆ°è¾¾æ—¶é—´
            alpha: å¯é æ€§å‚æ•°
            K: å€™é€‰è·¯å¾„æ•°é‡
            max_labels: æœ€å¤§æ ‡ç­¾æ•°
            print_interval: æ‰“å°é—´éš”
        
        Returns:
            åŒ…å«Kæ¡å€™é€‰è·¯å¾„çš„ç»“æœå­—å…¸
        """
        
        print(f"\n{'='*70}")
        print(f"åå‘Label-Settingæ±‚è§£ï¼ˆK-Pathsç‰ˆæœ¬ï¼‰")
        print(f"{'='*70}")
        print(f"  èµ·ç‚¹: {origin}")
        print(f"  ç»ˆç‚¹: {destination}")
        print(f"  ç›®æ ‡åˆ°è¾¾:  {target_arrival_time/10:.1f}åˆ†")
        print(f"  å¯é æ€§: Î±={alpha*100:.1f}%")
        print(f"  å€™é€‰è·¯å¾„æ•°:  K={K}")
        print(f"{'='*70}\n")
        
        start_time = time.time()
        
        # åˆå§‹åŒ–
        open_labels = []
        node_labels = defaultdict(list)
        origin_candidates = []  # âœ… å­˜å‚¨æ‰€æœ‰åˆ°è¾¾èµ·ç‚¹çš„å€™é€‰æ ‡ç­¾
        
        # åˆå§‹æ ‡ç­¾
        init_dist = AlphaDiscreteDistribution([target_arrival_time] * self.L1, self.L1)
        init_label = ReverseLabel(destination, init_dist, [destination], target_arrival_time)
        
        heapq.heappush(open_labels, init_label)
        node_labels[destination].append(init_label)
        self.stats = defaultdict(int)
        self.stats['labels_generated'] = 1
        
        print(f"å¼€å§‹æœç´¢ K={K} æ¡å€™é€‰è·¯å¾„...\n")
        
        iteration = 0
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ä¸»å¾ªç¯ï¼šæ‰¾åˆ°Kæ¡åˆ°è¾¾èµ·ç‚¹çš„è·¯å¾„
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        while open_labels and self.stats['labels_generated'] < max_labels:
            iteration += 1
            current_label = heapq.heappop(open_labels)
            
            if self.verbose and (iteration % print_interval == 0 or iteration <= 5):
                print(f"  è¿­ä»£#{iteration}: èŠ‚ç‚¹{current_label.node_id}, "
                      f"cost={current_label.cost/10:.1f}åˆ†, "
                      f"å€™é€‰æ•°={len(origin_candidates)}")
            
            # âœ… åˆ°è¾¾èµ·ç‚¹ï¼šä¿å­˜ä¸ºå€™é€‰è·¯å¾„
            if current_label.node_id == origin:
                latest_departure = current_label.distribution.get_quantile(1 - alpha)
                expected_departure = current_label.mean_cache
                
                # ä¿å­˜å€™é€‰è·¯å¾„ä¿¡æ¯
                candidate_info = {
                    'iteration': iteration,
                    'path': list(reversed(current_label.path)),
                    'distribution': current_label.distribution,
                    'latest_departure': latest_departure,
                    'expected_departure': expected_departure,
                    'median_departure': current_label.distribution.get_median(),
                    'std_departure': np.sqrt(current_label.variance_cache),
                    'variance': current_label.variance_cache,
                    'label': current_label,
                    'alpha': alpha,
                    'rank': None,
                    'is_best':  False  # â† æ·»åŠ è¿™ä¸ªå­—æ®µ
                }
                
                origin_candidates.append(candidate_info)
                
                print(f"  ğŸ¯ æ‰¾åˆ°å€™é€‰è·¯å¾„#{len(origin_candidates)}  è¿­ä»£#{iteration}, "
                      f"Q_{{1-Î±}}={latest_departure/10:.1f}åˆ†, "
                      f"Mean={expected_departure/10:.1f}åˆ†, "
                      f"è·¯å¾„é•¿åº¦={len(current_label.path)}")
                
                # âœ… æ‰¾åˆ°Kæ¡è·¯å¾„åç»§ç»­æœç´¢ï¼ˆç¡®ä¿æ¢ç´¢å……åˆ†ï¼‰
                if len(origin_candidates) >= K:
                    # å¯ä»¥é€‰æ‹©ï¼š
                    # é€‰é¡¹Aï¼šç«‹å³åœæ­¢ï¼ˆå¿«é€Ÿï¼‰
                    # é€‰é¡¹Bï¼šç»§ç»­æœç´¢ä¸€æ®µæ—¶é—´ï¼ˆæ›´å…¨é¢ï¼‰
                    
                    # è¿™é‡Œä½¿ç”¨é€‰é¡¹Bï¼šç»§ç»­æœç´¢ï¼Œä½†æœ‰ä¸Šé™
                    if len(origin_candidates) >= K * 2:  # æ‰¾åˆ°2Kæ¡ååœæ­¢
                        print(f"\n  âœ“ å·²æ‰¾åˆ° {len(origin_candidates)} æ¡å€™é€‰è·¯å¾„ï¼Œåœæ­¢æœç´¢\n")
                        break
                
                # ç»§ç»­æœç´¢å…¶ä»–è·¯å¾„
                continue
            
            # æ”¯é…æ€§æ£€æŸ¥ï¼ˆè¾ƒå®½æ¾ï¼Œä¿ç•™å¤šæ ·æ€§ï¼‰
            if self._is_dominated(current_label, node_labels[current_label.node_id], alpha):
                self.stats['labels_dominated'] += 1
                continue
            
            self.stats['labels_extended'] += 1
            
            # åå‘æ‰©å±•
            if current_label.node_id not in self.reverse_adj_list:
                continue
            
            for predecessor in self.reverse_adj_list[current_label.node_id]:
                if predecessor in current_label.path:
                    continue
                
                # åå‘å·ç§¯
                try:
                    def get_link_dist(u, v, slot):
                        return self._get_link_distribution_at_slot(u, v, slot)
                    
                    get_link_dist.__self__ = self
                    
                    new_dist = current_label.distribution.reverse_convolve(
                        get_link_dist_func=get_link_dist,
                        predecessor=predecessor,
                        current=current_label.node_id,
                        time_intervals_per_day=self.time_intervals_per_day,
                        L2=self.L2
                    )
                    
                    self.stats['convolutions'] += 1
                    
                except Exception as e:
                    if self.verbose and iteration <= 10:
                        print(f"      âš  å·ç§¯å¤±è´¥: {e}")
                    continue
                
                new_cost = new_dist.get_quantile(1 - alpha)
                new_label = ReverseLabel(predecessor, new_dist, 
                                        current_label.path + [predecessor], new_cost)
                
                self.stats['labels_generated'] += 1
                
                # æ”¯é…æ€§å‰ªæ
                if self._is_dominated(new_label, node_labels[predecessor], alpha):
                    self.stats['labels_dominated'] += 1
                    continue
                
                # åå‘å‰ªæ
                original_count = len(node_labels[predecessor])
                node_labels[predecessor] = [
                    old for old in node_labels[predecessor]
                    if not new_label.dominates_weak(old, alpha)
                ]
                self.stats['labels_dominated'] += (original_count - len(node_labels[predecessor]))
                
                node_labels[predecessor].append(new_label)
                node_labels[predecessor] = self._prune_labels(node_labels[predecessor], alpha)
                heapq.heappush(open_labels, new_label)
            
            # è¿›åº¦æ˜¾ç¤º
            if not self.verbose and iteration % 100 == 0:
                print(f"  è¿›åº¦: è¿­ä»£#{iteration}, ç”Ÿæˆ{self.stats['labels_generated']: ,}, "
                      f"å€™é€‰{len(origin_candidates)}, "
                      f"å‰ªæ{self.stats['labels_dominated']:,}", end='\r')
        
        total_time = time.time() - start_time
        
        print(f"\n\n{'='*70}")
        print(f"æœç´¢å®Œæˆ")
        print(f"{'='*70}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤2ï¼šå¯¹Kæ¡å€™é€‰è·¯å¾„æ’åº
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        if not origin_candidates:
            print(f"âœ— æœªæ‰¾åˆ°åˆ°è¾¾èµ·ç‚¹çš„è·¯å¾„")
            return {
                'success': False,
                'total_time': total_time,
                'iterations': iteration,
                'stats':  dict(self.stats),
                'num_candidates': 0
            }
        
        print(f"\næ‰¾åˆ° {len(origin_candidates)} æ¡å€™é€‰è·¯å¾„")
        print(f"å¼€å§‹æ’åºå’Œæ¯”è¾ƒ...\n")
        
        # âœ… å¤šç›®æ ‡æ’åºï¼šä¸»è¦Q_{1-Î±}ï¼Œæ¬¡è¦Meanï¼Œå†æ¬¡è¦-Var
        def rank_score(candidate):
            return (
                candidate['latest_departure'],      # ä¸»ç›®æ ‡ï¼šQ_{1-Î±}ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
                candidate['expected_departure'],    # æ¬¡è¦ï¼šå‡å€¼ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
                -candidate['variance']              # å†æ¬¡è¦ï¼šæ–¹å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            )
        
        # æ’åºï¼šä»æœ€ä¼˜åˆ°æœ€å·®
        sorted_candidates = sorted(origin_candidates, key=rank_score, reverse=True)
        
        # è®¾ç½®æ’å
        for rank, candidate in enumerate(sorted_candidates, 1):
            candidate['rank'] = rank
            candidate['is_best'] = (rank == 1)  # â† æ’åç¬¬1çš„æ ‡è®°ä¸ºæœ€ä¼˜
        
        # å–å‰Kæ¡
        top_k_candidates = sorted_candidates[:K]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤3ï¼šè¾“å‡ºç»“æœ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        best_candidate = top_k_candidates[0]
        
        print(f"{'='*70}")
        print(f"Top-{len(top_k_candidates)} å€™é€‰è·¯å¾„å¯¹æ¯”")
        print(f"{'='*70}\n")
        
        print(f"{'æ’å':<6} {'Q_{{1-Î±}}(åˆ†)':<15} {'Mean(åˆ†)':<15} {'Std(åˆ†)':<12} {'è·¯å¾„é•¿åº¦':<10}")
        print(f"{'-'*70}")
        
        for candidate in top_k_candidates: 
            print(f"{candidate['rank']:<6} "
                  f"{candidate['latest_departure']/10:<15.1f} "
                  f"{candidate['expected_departure']/10:<15.1f} "
                  f"{candidate['std_departure']/10:<12.2f} "
                  f"{len(candidate['path']):<10}")
        
        print(f"\n{'='*70}")
        print(f"âœ“ æœ€ä¼˜è·¯å¾„ï¼ˆæ’å#1ï¼‰")
        print(f"{'='*70}")
        print(f"\n  è·¯å¾„:  {self._format_path(best_candidate['path'])}")
        print(f"  é•¿åº¦: {len(best_candidate['path'])} ä¸ªèŠ‚ç‚¹")
        print(f"\n  æ—¶é—´:")
        print(f"    ç›®æ ‡åˆ°è¾¾: {target_arrival_time/10:.1f}åˆ†")
        print(f"    æœ€æ™šå‡ºå‘ (Î±={alpha}): {best_candidate['latest_departure']/10:.1f}åˆ†")
        print(f"    æœŸæœ›å‡ºå‘: {best_candidate['expected_departure']/10:.1f}åˆ†")
        print(f"    é¢„ç•™æ—¶é—´: {(target_arrival_time - best_candidate['latest_departure'])/10:.1f}åˆ†")
        print(f"    æ ‡å‡†å·®:  {best_candidate['std_departure']/10:.2f}åˆ†")
        print(f"\n  æ€§èƒ½:")
        print(f"    æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"    è¿­ä»£æ¬¡æ•°: {iteration}")
        print(f"    å€™é€‰è·¯å¾„æ•°: {len(origin_candidates)}")
        print(f"    ç”Ÿæˆæ ‡ç­¾:  {self.stats['labels_generated']: ,}")
        print(f"    å‰ªæç‡: {self.stats['labels_dominated']/self.stats['labels_generated']*100:.1f}%")
        print(f"{'='*70}\n")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ„å»ºè¿”å›ç»“æœ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        result = {
            'success': True,
            # æœ€ä¼˜è·¯å¾„ä¿¡æ¯
            'path': best_candidate['path'],
            'latest_departure_time': best_candidate['latest_departure'],
            'expected_departure_time': best_candidate['expected_departure'],
            'median_departure_time': best_candidate['median_departure'],
            'std_departure_time': best_candidate['std_departure'],
            'reserved_time': target_arrival_time - best_candidate['latest_departure'],
            'distribution': best_candidate['distribution'],
            
            # Top-Kå€™é€‰è·¯å¾„
            'top_k_candidates': top_k_candidates,
            'num_candidates': len(origin_candidates),
            'all_candidates': sorted_candidates,
            
            # å…ƒä¿¡æ¯
            'total_time':  total_time,
            'iterations': iteration,
            'alpha': alpha,
            'K':  K,
            'origin':  origin,
            'destination': destination,
            'target_arrival_time': target_arrival_time,
            'stats': dict(self.stats)
        }
        
        return result

    def solve(self, origin: int, destination: int, target_arrival_time: int,
            alpha: float, max_labels: int = 100000, K:int=5,
            print_interval: int = 100,
            save_all_paths: bool = True) -> Dict: 
        """
        æ ‡å‡†solveæ¥å£ï¼ˆå…¼å®¹åŸä»£ç ï¼‰
        
        å†…éƒ¨è°ƒç”¨solve_k_pathsï¼ŒK=5ï¼ˆè¿”å›5æ¡å€™é€‰è·¯å¾„ï¼‰
        """
        result = self.solve_k_paths(
            origin, destination, target_arrival_time, alpha,
            K=K, max_labels=max_labels, print_interval=print_interval
        )
        
        # æ·»åŠ all_pathsç”¨äºå…¼å®¹
        if save_all_paths and 'all_candidates' in result:
            result['all_paths'] = result['all_candidates']
            result['num_candidate_paths'] = result['num_candidates']
        
        return result
    
    def _is_dominated(self, label: ReverseLabel, existing_labels: List[ReverseLabel], 
                     alpha: float) -> bool:
        """
        æ”¯é…æ€§æ£€æŸ¥ï¼ˆä¿å®ˆç‰ˆæœ¬ï¼‰
        
        ç­–ç•¥ï¼š
        1.å¦‚æœèŠ‚ç‚¹æ ‡ç­¾æ•° < max_labels_per_nodeï¼šä¸å‰ªæ
        2.å¦‚æœå·²æ»¡ï¼šæ£€æŸ¥æ˜¯å¦è¢«å¼±æ”¯é…
        """
        # ç­–ç•¥1ï¼šä¿ç•™å¤šæ ·æ€§
        if len(existing_labels) < self.max_labels_per_node:
            # åªæœ‰è¢«å¤šä¸ªæ ‡ç­¾æ˜ç¡®æ”¯é…æ—¶æ‰å‰ªæ
            domination_count = 0
            for existing in existing_labels:
                if existing.dominates_weak(label, alpha):
                    domination_count += 1
            
            # è¢«2ä¸ªä»¥ä¸Šæ ‡ç­¾æ”¯é…æ‰å‰ªæ
            return domination_count >= 2
        
        # ç­–ç•¥2ï¼šæ ‡ç­¾æ•°å·²æ»¡ï¼Œä½¿ç”¨å¼±æ”¯é…
        for existing in existing_labels:
            if existing.dominates_weak(label, alpha):
                return True
        
        return False
    
    def _prune_labels(self, labels: List[ReverseLabel], alpha: float) -> List[ReverseLabel]:
        """
        å½“æ ‡ç­¾æ•°è¶…é™æ—¶ï¼Œç§»é™¤æœ€å·®çš„æ ‡ç­¾
        
        æ’åºæ ‡å‡†ï¼š
        1.Q_{1-Î±}ï¼ˆä¸»è¦ï¼‰
        2.Meanï¼ˆæ¬¡è¦ï¼‰
        3.-Varï¼ˆå†æ¬¡è¦ï¼‰
        """
        if len(labels) <= self.max_labels_per_node:
            return labels
        
        # å¤šç›®æ ‡æ’åº
        def label_score(label):
            q = label.distribution.get_quantile(1 - alpha)
            return (q, label.mean_cache, -label.variance_cache)
        
        # ä¿ç•™æœ€ä¼˜çš„max_labels_per_nodeä¸ª
        sorted_labels = sorted(labels, key=label_score, reverse=True)
        return sorted_labels[:self.max_labels_per_node]

    def _format_path(self, path: List[int]) -> str:
        if len(path) <= 10:
            return ' â†’ '.join(map(str, path))
        return f"{' â†’ '.join(map(str, path[:5]))} â†’ ...â†’ {' â†’ '.join(map(str, path[-3:]))}"