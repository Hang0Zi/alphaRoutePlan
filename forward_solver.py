"""
æ­£å‘Label-Settingæ±‚è§£å™¨ï¼ˆä¿®å¤ Î± æ•æ„Ÿæ€§ï¼‰
ç»™å®šå‡ºå‘æ—¶é—´å’Œèµ·ç»ˆç‚¹ï¼Œæ¨å¯¼åˆ°è¾¾æ—¶é—´åˆ†å¸ƒå’ŒÎ±æ¦‚ç‡ä¸‹æœ€æ—©åˆ°è¾¾æ—¶é—´çš„è·¯å¾„
"""

import numpy as np
import heapq
import time
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass, field


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®ç»“æ„å®šä¹‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


from dataclasses import dataclass, field
from typing import List, Optional
import numpy as np


@dataclass
class ForwardDiscreteDistribution:
    """
    æ­£å‘ç¦»æ•£åˆ†å¸ƒç±»(åˆ°è¾¾æ—¶é—´åˆ†å¸ƒ)
    
    æ”¯æŒå¸¦æƒé‡çš„ç¦»æ•£åˆ†å¸ƒ,ç”¨äºæ›´å‡†ç¡®åœ°è¡¨ç¤ºåˆ°è¾¾æ—¶é—´çš„æ¦‚ç‡åˆ†å¸ƒ
    """
    values: np.ndarray  # æ”¹ä¸º numpy æ•°ç»„
    L1: int
    weights: Optional[np.ndarray] = None  # âœ… æ·»åŠ æƒé‡å‚æ•°
    
    def __init__(self, values, L1, weights=None):
        """
        åˆå§‹åŒ–æ­£å‘ç¦»æ•£åˆ†å¸ƒ
        
        Args:
            values: ç¦»æ•£å€¼åˆ—è¡¨æˆ–æ•°ç»„
            L1: ç¦»æ•£åŒ–çº§åˆ«
            weights: æƒé‡æ•°ç»„(å¯é€‰),å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å‡åŒ€æƒé‡
        """
        # è½¬æ¢ä¸º numpy æ•°ç»„
        if not isinstance(values, np.ndarray):
            values = np.array(values)
        
        if len(values) != L1:
            raise ValueError(f"æœŸæœ›{L1}ä¸ªå€¼,å®é™…å¾—åˆ°{len(values)}ä¸ª")
        
        self.L1 = L1
        
        # âœ… å¤„ç†æƒé‡
        if weights is None:
            # é»˜è®¤å‡åŒ€æƒé‡
            self.weights = np.ones(L1) / L1
        else:
            if not isinstance(weights, np.ndarray):
                weights = np.array(weights)
            
            if len(weights) != L1:
                raise ValueError(f"æƒé‡æ•°é‡({len(weights)})ä¸å€¼æ•°é‡({L1})ä¸åŒ¹é…")
            
            # å½’ä¸€åŒ–æƒé‡
            if weights.sum() > 0:
                self.weights = weights / weights.sum()
            else:
                self.weights = np.ones(L1) / L1
        
        # æ’åºå€¼å’Œå¯¹åº”çš„æƒé‡
        sorted_indices = np.argsort(values)
        self.values = values[sorted_indices]
        self.weights = self.weights[sorted_indices]
    
    def get_quantile(self, alpha: float) -> float:
        """
        è·å–Î±åˆ†ä½æ•°(ä½¿ç”¨æƒé‡çš„ç´¯ç§¯åˆ†å¸ƒ)
        
        Args:
            alpha: åˆ†ä½æ•°(0-1)
            
        Returns:
            å¯¹åº”çš„åˆ†ä½æ•°å€¼
        """
        if alpha <= 0:
            return float(self.values[0])
        if alpha >= 1:
            return float(self.values[-1])
        
        # âœ… ä½¿ç”¨åŠ æƒç´¯ç§¯åˆ†å¸ƒ
        cumsum = np.cumsum(self.weights)
        
        # çº¿æ€§æ’å€¼
        quantile_value = np.interp(alpha, cumsum, self.values)
        
        return float(quantile_value)
    
    def get_mean(self) -> float:
        """è®¡ç®—åŠ æƒå¹³å‡å€¼"""
        return float(np.sum(self.values * self.weights))
    
    def get_expected(self) -> float:
        """è®¡ç®—æœŸæœ›å€¼(ä¸ get_mean ç›¸åŒ)"""
        return self.get_mean()
    
    def get_variance(self) -> float:
        """è®¡ç®—åŠ æƒæ–¹å·®"""
        mu = self.get_mean()
        return float(np.sum(self.weights * (self.values - mu) ** 2))
    
    def get_std(self) -> float:
        """è®¡ç®—åŠ æƒæ ‡å‡†å·®"""
        return float(np.sqrt(self.get_variance()))
    
    def get_median(self) -> float:
        """è®¡ç®—ä¸­ä½æ•°(0.5åˆ†ä½æ•°)"""
        return self.get_quantile(0.5)
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼(ç”¨äºJSONåºåˆ—åŒ–)"""
        return {
            'values':  self.values.tolist() if isinstance(self.values, np.ndarray) else list(self.values),
            'weights': self.weights.tolist() if isinstance(self.weights, np.ndarray) else list(self.weights),
            'L1': self.L1
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'ForwardDiscreteDistribution':
        """ä»å­—å…¸åˆ›å»ºåˆ†å¸ƒ"""
        values = np.array(data['values'])
        L1 = data.get('L1', len(values))
        weights = np.array(data['weights']) if 'weights' in data else None
        return cls(values, L1, weights)
    
    def forward_convolve(self,
                        get_link_dist_func,
                        current: int,
                        successor: int,
                        time_intervals_per_day: int,
                        L2: int,
                        K:  int) -> 'ForwardDiscreteDistribution':
        """
        æ­£å‘å·ç§¯: ä»å½“å‰èŠ‚ç‚¹çš„å‡ºå‘æ—¶é—´åˆ†å¸ƒæ¨å¯¼åˆ°åç»§èŠ‚ç‚¹çš„åˆ°è¾¾æ—¶é—´åˆ†å¸ƒ
        
        æ ¸å¿ƒæ€æƒ³:
        1.éå†æ‰€æœ‰å¯èƒ½çš„å‡ºå‘æ—¶é—´ t_dep(ä»self.values)
        2.å¯¹æ¯ä¸ª t_dep,è®¡ç®—å…¶å¯¹åº”çš„æ—¶é—´ç‰‡ slot_dep
        3.ä»è·¯æ®µåˆ†å¸ƒ D(current, successor, slot_dep) é‡‡æ ·L2ä¸ªæ—…è¡Œæ—¶é—´
        4.è®¡ç®— L1*L2 ä¸ªåˆ°è¾¾æ—¶é—´: t_arr = t_dep + travel_time
        5.å–å‰Kä¸ªä½œä¸ºæ–°çš„åˆ°è¾¾æ—¶é—´åˆ†å¸ƒ
        
        Args:
            get_link_dist_func: è·å–é“¾è·¯åˆ†å¸ƒçš„å‡½æ•°
            current: å½“å‰èŠ‚ç‚¹
            successor: åç»§èŠ‚ç‚¹
            time_intervals_per_day: æ¯å¤©æ—¶é—´ç‰‡æ•°
            L2: æ¯ä¸ªå‡ºå‘æ—¶é—´é‡‡æ ·çš„æ—…è¡Œæ—¶é—´æ•°
            K: å–å‰Kä¸ªåˆ°è¾¾æ—¶é—´
            
        Returns:
            åˆ°è¾¾æ—¶é—´åˆ†å¸ƒ
        """
        
        # è·å–å¯ç”¨æ—¶é—´ç‰‡
        available_slots = self._get_available_slots(get_link_dist_func, current, successor)
        
        if not available_slots:
            raise ValueError(f"è¾¹({current}, {successor})æ²¡æœ‰é“¾è·¯åˆ†å¸ƒæ•°æ®")
        
        # âœ… å­˜å‚¨æ‰€æœ‰å€™é€‰åˆ°è¾¾æ—¶é—´åŠå…¶æƒé‡
        all_arrival_times = []
        all_arrival_weights = []
        
        # éå†æ‰€æœ‰å‡ºå‘æ—¶é—´
        for i, t_dep in enumerate(self.values):
            # è¯¥å‡ºå‘æ—¶é—´çš„æƒé‡
            dep_weight = self.weights[i]
            
            # è®¡ç®—å‡ºå‘æ—¶é—´ç‰‡
            slot_dep = int(t_dep / 10) % time_intervals_per_day
            
            # è·å–è¯¥æ—¶é—´ç‰‡çš„è·¯æ®µåˆ†å¸ƒ
            D_slot = get_link_dist_func(current, successor, slot_dep)
            
            if D_slot is None:
                # å¦‚æœç²¾ç¡®æ—¶é—´ç‰‡æ²¡æœ‰åˆ†å¸ƒ,å°è¯•æœ€è¿‘çš„æ—¶é—´ç‰‡
                nearest_slot = self._find_nearest_slot(
                    slot_dep, available_slots, time_intervals_per_day
                )
                D_slot = get_link_dist_func(current, successor, nearest_slot)
            
            if D_slot is None:
                continue
            
            # ä»è·¯æ®µåˆ†å¸ƒé‡‡æ ·L2ä¸ªæ—…è¡Œæ—¶é—´
            sampled_travel_times = D_slot.sample_L2_times(t_dep, L2)
            
            # âœ… è®¡ç®—å¯¹åº”çš„åˆ°è¾¾æ—¶é—´å’Œæƒé‡
            for travel_time in sampled_travel_times: 
                t_arr = t_dep + travel_time
                all_arrival_times.append(t_arr)
                # æƒé‡æŒ‰å‡ºå‘æ—¶é—´æƒé‡å’Œé‡‡æ ·æ•°å¹³å‡åˆ†é…
                all_arrival_weights.append(dep_weight / L2)
        
        if not all_arrival_times: 
            raise ValueError(f"æ­£å‘å·ç§¯å¤±è´¥: æ— æœ‰æ•ˆåˆ°è¾¾æ—¶é—´")
        
        # âœ… è½¬æ¢ä¸º numpy æ•°ç»„å¹¶æ’åº
        all_arrival_times = np.array(all_arrival_times)
        all_arrival_weights = np.array(all_arrival_weights)
        
        # æŒ‰åˆ°è¾¾æ—¶é—´æ’åº
        sorted_indices = np.argsort(all_arrival_times)
        all_arrival_times = all_arrival_times[sorted_indices]
        all_arrival_weights = all_arrival_weights[sorted_indices]
        
        # âœ… é™é‡‡æ ·åˆ°Kä¸ªå€¼(ä¿ç•™æƒé‡ä¿¡æ¯)
        if len(all_arrival_times) > K:
            # ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•é€‰æ‹©ä»£è¡¨æ€§çš„Kä¸ªç‚¹
            cumsum = np.cumsum(all_arrival_weights)
            cumsum = cumsum / cumsum[-1]  # å½’ä¸€åŒ–
            
            # é€‰æ‹©Kä¸ªå‡åŒ€åˆ†å¸ƒçš„åˆ†ä½æ•°
            target_quantiles = np.linspace(1/(K+1), K/(K+1), K)
            
            # é€šè¿‡æ’å€¼æ‰¾åˆ°å¯¹åº”çš„åˆ°è¾¾æ—¶é—´
            selected_times = np.interp(target_quantiles, cumsum, all_arrival_times)
            selected_weights = np.ones(K) / K  # é™é‡‡æ ·åä½¿ç”¨å‡åŒ€æƒé‡
        else: 
            selected_times = all_arrival_times
            selected_weights = all_arrival_weights
        
        # âœ… è°ƒæ•´åˆ°L1ä¸ªå€¼
        if len(selected_times) < self.L1:
            # æ’å€¼å¢åŠ åˆ°L1ä¸ª
            cumsum = np.cumsum(selected_weights)
            cumsum = cumsum / cumsum[-1]
            target_quantiles = np.linspace(1/(self.L1+1), self.L1/(self.L1+1), self.L1)
            final_times = np.interp(target_quantiles, cumsum, selected_times)
            final_weights = np.ones(self.L1) / self.L1
        elif len(selected_times) > self.L1:
            # é™é‡‡æ ·åˆ°L1ä¸ª
            cumsum = np.cumsum(selected_weights)
            cumsum = cumsum / cumsum[-1]
            target_quantiles = np.linspace(1/(self.L1+1), self.L1/(self.L1+1), self.L1)
            final_times = np.interp(target_quantiles, cumsum, selected_times)
            final_weights = np.ones(self.L1) / self.L1
        else:
            # å¤§å°åˆšå¥½
            final_times = selected_times
            final_weights = selected_weights
        
        # âœ… ç¡®ä¿æ’åº
        sorted_indices = np.argsort(final_times)
        final_times = final_times[sorted_indices]
        final_weights = final_weights[sorted_indices]
        
        # âœ… åˆ›å»ºæ–°åˆ†å¸ƒ(ä¼ å…¥æƒé‡)
        return ForwardDiscreteDistribution(final_times, self.L1, final_weights)
    
    def _find_nearest_slot(self, target_slot: int, available_slots: List[int],
                          time_intervals_per_day:  int) -> int:
        """æ‰¾åˆ°æœ€è¿‘çš„æ—¶é—´ç‰‡"""
        min_dist = float('inf')
        best_slot = available_slots[0]
        
        for slot in available_slots: 
            dist = abs(slot - target_slot)
            cyclic_dist = min(dist, time_intervals_per_day - dist)
            
            if cyclic_dist < min_dist:
                min_dist = cyclic_dist
                best_slot = slot
        
        return best_slot
    
    def _get_available_slots(self, get_link_dist_func, u: int, v: int) -> List[int]:
        """è·å–å¯ç”¨æ—¶é—´ç‰‡(å¸¦ç¼“å­˜)"""
        if not hasattr(ForwardDiscreteDistribution, '_slot_cache'):
            ForwardDiscreteDistribution._slot_cache = {}
        
        cache_key = (u, v)
        if cache_key in ForwardDiscreteDistribution._slot_cache:
            return ForwardDiscreteDistribution._slot_cache[cache_key]
        
        available = []
        try:
            link_distributions = get_link_dist_func.__self__.link_distributions
            for (link_u, link_v, slot) in link_distributions.keys():
                if link_u == u and link_v == v:
                    available.append(slot)
        except AttributeError:
            raise ValueError("æ— æ³•è®¿é—®é“¾è·¯åˆ†å¸ƒæ•°æ®")
        
        result = sorted(set(available))
        ForwardDiscreteDistribution._slot_cache[cache_key] = result
        return result
    
    def __len__(self) -> int:
        """è¿”å›åˆ†å¸ƒå¤§å°"""
        return self.L1
    
    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"ForwardDiscreteDistribution(L1={self.L1}, "
                f"mean={self.get_mean():.2f}, "
                f"std={self.get_std():.2f}, "
                f"range=[{self.values[0]:.1f}, {self.values[-1]:.1f}])")

@dataclass
class LinkTimeDistribution:
    """è·¯æ®µæ—…è¡Œæ—¶é—´åˆ†å¸ƒ"""
    time_prob: Dict[int, float]
    times: List[int]
    cdf: List[float]
    time_slot: int
    
    def __init__(self, time_prob_dict: Dict[int, float], time_slot: int = None):
        if not time_prob_dict:
            raise ValueError("é“¾è·¯åˆ†å¸ƒä¸èƒ½ä¸ºç©º")
        
        total_prob = sum(time_prob_dict.values())
        self.time_prob = {t: p/total_prob for t, p in time_prob_dict.items()}
        self.time_slot = time_slot
        
        sorted_times = sorted(self.time_prob.keys())
        self.times = sorted_times
        
        cumulative = 0.0
        self.cdf = []
        for t in sorted_times:
            cumulative += self.time_prob[t]
            self.cdf.append(cumulative)
    
    def sample_L2_times(self, reference_time: int, L2: int) -> List[int]:
        """é‡‡æ ·L2ä¸ªæ—…è¡Œæ—¶é—´ï¼ˆé€†CDFæ–¹æ³•ï¼‰"""
        samples = []
        for i in range(1, L2 + 1):
            quantile = i / (L2 + 1)
            sample = self._inverse_cdf(quantile)
            samples.append(sample)
        return sorted(samples)
    
    def _inverse_cdf(self, quantile: float) -> int:
        """é€†CDFï¼ˆçº¿æ€§æ’å€¼ï¼‰"""
        if quantile <= 0:
            return self.times[0]
        if quantile >= 1:
            return self.times[-1]
        
        for i, cdf_val in enumerate(self.cdf):
            if cdf_val >= quantile:
                if i == 0:
                    return self.times[0]
                
                lower_cdf = self.cdf[i-1] if i > 0 else 0
                upper_cdf = cdf_val
                lower_time = self.times[i-1] if i > 0 else self.times[0]
                upper_time = self.times[i]
                
                if upper_cdf > lower_cdf:
                    weight = (quantile - lower_cdf) / (upper_cdf - lower_cdf)
                else:
                    weight = 0.5
                
                return int(round(lower_time + weight * (upper_time - lower_time)))
        
        return self.times[-1]


from dataclasses import dataclass, field
from typing import List
import numpy as np


@dataclass
class ForwardLabel:
    """æ­£å‘æ ‡ç­¾ç±»"""
    node_id: int
    distribution:  'ForwardDiscreteDistribution'
    path: List[int]
    cost: float
    alpha: float = 0.95  # å¯é æ€§å‚æ•°
    
    # âœ… æ·»åŠ ç¼“å­˜å­—æ®µ
    mean_cache: float = field(default=0.0, init=False, repr=False)
    variance_cache: float = field(default=0.0, init=False, repr=False)
    std_cache: float = field(default=0.0, init=False, repr=False)
    
    def __post_init__(self):
        """ååˆå§‹åŒ–: é¢„è®¡ç®—ç»Ÿè®¡é‡"""
        # âœ… é¢„è®¡ç®—å¹¶ç¼“å­˜ç»Ÿè®¡é‡
        self.mean_cache = self.distribution.get_mean()
        self.variance_cache = self.distribution.get_variance()
        self.std_cache = self.distribution.get_std()
    
    def __lt__(self, other:  'ForwardLabel') -> bool:
        """ä¼˜å…ˆé˜Ÿåˆ—æ’åº: costè¶Šå°è¶Šä¼˜"""
        return self.cost < other.cost
    
    # âœ… æ·»åŠ å±æ€§è®¿é—®å™¨
    @property
    def expected_value(self) -> float:
        """æœŸæœ›å€¼(å‡å€¼)"""
        return self.mean_cache
    
    @property
    def std_value(self) -> float:
        """æ ‡å‡†å·®"""
        return self.std_cache
    
    @property
    def variance_value(self) -> float:
        """æ–¹å·®"""
        return self.variance_cache
    
    def get_quantile(self, alpha: float) -> float:
        """è·å–Î±åˆ†ä½æ•°"""
        return self.distribution.get_quantile(alpha)
    
    def dominates_weak(self, other: 'ForwardLabel', alpha: float, epsilon: float = 1e-6) -> bool:
        """
        æ”¯é…è§„åˆ™(æ­£å‘: è¶Šå°è¶Šå¥½)
        
        æ”¯é…æ¡ä»¶(æŒ‰ä¼˜å…ˆçº§):
        1.Î±åˆ†ä½æ•°ä¸¥æ ¼æ›´å° â†’ æ”¯é…
        2.Î±åˆ†ä½æ•°ç›¸ç­‰ + æœŸæœ›å€¼æ›´å° â†’ æ”¯é…
        3.Î±åˆ†ä½æ•°å’ŒæœŸæœ›å€¼éƒ½ç›¸ç­‰ + æ–¹å·®æ›´å° â†’ æ”¯é…
        
        Args:
            other: å¦ä¸€ä¸ªæ ‡ç­¾
            alpha: å¯é æ€§å‚æ•°
            epsilon: æ•°å€¼å®¹å·®
            
        Returns:
            bool:  æ˜¯å¦æ”¯é…
        """
        # å¿…é¡»åœ¨åŒä¸€èŠ‚ç‚¹
        if self.node_id != other.node_id:
            return False
        
        # ä¸»ç›®æ ‡: Î±åˆ†ä½æ•°(è¶Šå°è¶Šå¥½)
        Q_self = self.distribution.get_quantile(alpha)
        Q_other = other.distribution.get_quantile(alpha)
        
        # ç­–ç•¥1:ä¸»ç›®æ ‡ä¸¥æ ¼æ›´ä¼˜
        if Q_self < Q_other - epsilon:
            return True
        
        # ç­–ç•¥2:ä¸»ç›®æ ‡ç›¸ç­‰,æ¯”è¾ƒæ¬¡è¦ç›®æ ‡
        if abs(Q_self - Q_other) <= epsilon:
            mu_self = self.expected_value
            mu_other = other.expected_value
            
            # æœŸæœ›å€¼æ›´ä¼˜
            if mu_self < mu_other - epsilon:
                return True
            
            # æœŸæœ›å€¼ä¹Ÿç›¸ç­‰,æ¯”è¾ƒæ–¹å·®
            if abs(mu_self - mu_other) <= epsilon:
                sigma2_self = self.variance_value
                sigma2_other = other.variance_value
                
                # æ–¹å·®æ›´å°(æ›´ç¨³å®š)
                if sigma2_self < sigma2_other - epsilon:
                    return True
        
        return False
    
    def dominates(self, other: 'ForwardLabel', alpha: float, epsilon: float = 1e-6) -> bool:
        """ç»Ÿä¸€æ¥å£"""
        return self.dominates_weak(other, alpha, epsilon)
    
    def __eq__(self, other: object) -> bool:
        """ç›¸ç­‰æ€§åˆ¤æ–­"""
        if not isinstance(other, ForwardLabel):
            return False
        return (self.node_id == other.node_id and 
                abs(self.cost - other.cost) < 1e-9)
    
    def __hash__(self) -> int:
        """å“ˆå¸Œå€¼"""
        return hash((self.node_id, round(self.cost, 6)))
    
    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"ForwardLabel(node={self.node_id}, "
                f"Q={self.cost:.2f}, "
                f"E={self.expected_value:.2f}, "
                f"Ïƒ={self.std_value:.2f})")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ­£å‘æ±‚è§£å™¨ä¸»ç±»
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ForwardLabelSettingSolver:
    """æ­£å‘Label-Settingæ±‚è§£å™¨"""
    
    def __init__(self, G, sparse_data, node_to_index, scenario_dates,
                 scenario_probs, time_intervals_per_day,
                 L1:  int = 50, L2: int = 10, K: int = 100,
                 verbose: bool = False,
                 max_labels_per_node: int = 20):
        """åˆå§‹åŒ–"""
        self.G = G
        self.sparse_data = sparse_data
        self.node_to_index = node_to_index
        self.index_to_node = {v:  k for k, v in node_to_index.items()}
        self.scenario_dates = scenario_dates
        self.scenario_probs = scenario_probs
        self.time_intervals_per_day = time_intervals_per_day
        self.n_scenarios = len(scenario_dates)
        
        self.L1 = L1
        self.L2 = L2
        self.K = K
        self.verbose = verbose
        self.max_labels_per_node = max_labels_per_node
        
        print(f"\n{'='*70}")
        print(f"åˆå§‹åŒ–æ­£å‘Label-Settingæ±‚è§£å™¨")
        print(f"{'='*70}")
        print(f"  ç®—æ³•:  æ­£å‘Label-Setting (Î±æ•æ„Ÿæ€§ä¿®å¤ç‰ˆ)")
        print(f"  é—®é¢˜:  ç»™å®šå‡ºå‘æ—¶é—´ï¼Œæ±‚è§£åˆ°è¾¾æ—¶é—´åˆ†å¸ƒ")
        print(f"  å‚æ•°: L1={L1}, L2={L2}, K={K}")
        print(f"  è¯¦ç»†è¾“å‡º: {'å¼€å¯' if verbose else 'å…³é—­'}")
        
        # æ„å»ºé‚»æ¥è¡¨
        self.adj_list = defaultdict(list)
        self._build_adjacency_lists()
        
        # é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ
        self.link_distributions = {}
        self._precompute_link_distributions()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = defaultdict(int)
        
        print(f"\nâœ“ åˆå§‹åŒ–å®Œæˆ")
        print(f"{'='*70}\n")
    
    def _build_adjacency_lists(self):
        """æ„å»ºé‚»æ¥è¡¨"""
        print(f"  [1/2] æ„å»ºé‚»æ¥è¡¨...")
        start_time = time.time()
        
        edges_set = set()
        for (scenario_idx, time_idx, from_idx, to_idx) in self.sparse_data.keys():
            if scenario_idx < self.n_scenarios:
                from_node = self.index_to_node[from_idx]
                to_node = self.index_to_node[to_idx]
                edges_set.add((from_node, to_node))
        
        for from_node, to_node in edges_set:
            self.adj_list[from_node].append(to_node)
        
        elapsed = time.time() - start_time
        print(f"      âœ“ å®Œæˆ (ç”¨æ—¶ {elapsed:.2f}s) - {len(edges_set):,} æ¡è¾¹")
    
    def _precompute_link_distributions(self):
        """é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ"""
        print(f"  [2/2] é¢„è®¡ç®—é“¾è·¯åˆ†å¸ƒ...")
        start_time = time.time()
        
        link_time_data = defaultdict(list)
        
        for (scenario_idx, time_idx, from_idx, to_idx), travel_time_minutes in self.sparse_data.items():
            if scenario_idx >= self.n_scenarios:
                continue
            
            from_node = self.index_to_node[from_idx]
            to_node = self.index_to_node[to_idx]
            travel_time_01min = int(travel_time_minutes * 10)
            
            link_time_data[(from_node, to_node, time_idx)].append(travel_time_01min)
        
        distribution_count = 0
        for (u, v, t), times in link_time_data.items():
            time_counts = defaultdict(int)
            for time_val in times:
                time_counts[time_val] += 1
            
            total = len(times)
            time_prob = {time_val: count/total for time_val, count in time_counts.items()}
            
            try:
                self.link_distributions[(u, v, t)] = LinkTimeDistribution(time_prob, time_slot=t)
                distribution_count += 1
            except ValueError:
                continue
        
        elapsed = time.time() - start_time
        print(f"      âœ“ å®Œæˆ (ç”¨æ—¶ {elapsed:.2f}s) - {distribution_count:,} ä¸ªåˆ†å¸ƒ")
    
    def _get_link_distribution_at_slot(self, u: int, v: int, slot: int) -> Optional[LinkTimeDistribution]:
        """è·å–æŒ‡å®šå‡ºå‘æ—¶é—´ç‰‡çš„é“¾è·¯åˆ†å¸ƒ"""
        if (u, v, slot) in self.link_distributions:
            return self.link_distributions[(u, v, slot)]
        
        # å®¹å·®åŒ¹é…
        tolerance = 5
        candidates = []
        
        for (link_u, link_v, link_t) in self.link_distributions.keys():
            if link_u == u and link_v == v:
                diff = abs(link_t - slot)
                cyclic_diff = min(diff, self.time_intervals_per_day - diff)
                
                if cyclic_diff <= tolerance:  
                    candidates.append((link_t, cyclic_diff))
        
        if candidates:
            best_slot = min(candidates, key=lambda x: x[1])[0]
            return self.link_distributions[(u, v, best_slot)]
        
        return None
    
    def solve_k_paths(self, origin:  int, destination: int, departure_time: int,
                    alpha: float, K: int = 10, max_labels: int = 100000,
                    print_interval: int = 100) -> Dict:
        """
        æ­£å‘K-Pathsæ±‚è§£ï¼šç»™å®šå‡ºå‘æ—¶é—´ï¼Œæ‰¾åˆ°Kæ¡å€™é€‰è·¯å¾„ï¼Œé€‰å‡ºÎ±åˆ†ä½æ•°åˆ°è¾¾æ—¶é—´æœ€æ—©çš„è·¯å¾„
        
        Args:  
            origin:  èµ·ç‚¹
            destination: ç»ˆç‚¹
            departure_time:  å‡ºå‘æ—¶é—´ï¼ˆ0.1åˆ†é’Ÿå•ä½ï¼‰
            alpha: å¯é æ€§å‚æ•°
            K: å€™é€‰è·¯å¾„æ•°é‡
            max_labels: æœ€å¤§æ ‡ç­¾æ•°
            print_interval: æ‰“å°é—´éš”
        
        Returns:
            åŒ…å«Kæ¡å€™é€‰è·¯å¾„å’Œæœ€ä¼˜è·¯å¾„çš„ç»“æœå­—å…¸
        """
        
        print(f"\n{'='*70}")
        print(f"æ­£å‘Label-Settingæ±‚è§£ï¼ˆK-Pathsç‰ˆæœ¬ï¼‰")
        print(f"{'='*70}")
        print(f"  èµ·ç‚¹: {origin}")
        print(f"  ç»ˆç‚¹: {destination}")
        print(f"  å‡ºå‘æ—¶é—´: {departure_time/10:.1f}åˆ† ({self._time_to_string(departure_time)})")
        print(f"  å¯é æ€§:  Î±={alpha*100:.1f}%")
        print(f"  å€™é€‰è·¯å¾„æ•°: K={K}")
        print(f"{'='*70}\n")
        
        start_time = time.time()
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤1ï¼šæœç´¢Kæ¡å€™é€‰è·¯å¾„
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        # åˆå§‹åŒ–
        open_labels = []
        node_labels = defaultdict(list)
        destination_candidates = []  # âœ… å­˜å‚¨æ‰€æœ‰åˆ°è¾¾ç»ˆç‚¹çš„å€™é€‰æ ‡ç­¾
        
        # âœ… åˆå§‹æ ‡ç­¾
        init_dist = ForwardDiscreteDistribution(
            values=np.array([departure_time] * self.L1),
            L1=self.L1,
            weights=np.ones(self.L1) / self.L1
        )
        init_label = ForwardLabel(origin, init_dist, [origin], departure_time)
        
        heapq.heappush(open_labels, init_label)
        node_labels[origin].append(init_label)
        self.stats = defaultdict(int)
        self.stats['labels_generated'] = 1
        
        print(f"å¼€å§‹æœç´¢ K={K} æ¡å€™é€‰è·¯å¾„...\n")
        
        iteration = 0
        
        # ä¸»å¾ªç¯ï¼šæ‰¾åˆ°Kæ¡åˆ°è¾¾ç»ˆç‚¹çš„è·¯å¾„
        while open_labels and self.stats['labels_generated'] < max_labels: 
            iteration += 1
            current_label = heapq.heappop(open_labels)
            
            if self.verbose and (iteration % print_interval == 0 or iteration <= 5):
                print(f"  è¿­ä»£#{iteration}:  èŠ‚ç‚¹{current_label.node_id}, "
                    f"cost={current_label.cost/10:.1f}åˆ†, "
                    f"å€™é€‰æ•°={len(destination_candidates)}")
            
            # âœ… åˆ°è¾¾ç»ˆç‚¹ï¼šä¿å­˜ä¸ºå€™é€‰è·¯å¾„
            if current_label.node_id == destination:
                earliest_arrival = current_label.distribution.get_quantile(alpha)
                expected_arrival = current_label.expected_value
                
                # âœ… è·å–è·¯å¾„åæ ‡ï¼ˆç”¨äºåœ°å›¾æ˜¾ç¤ºï¼‰
                path_coords = self._get_path_coordinates(current_label.path)
                
                # ä¿å­˜å€™é€‰è·¯å¾„ä¿¡æ¯
                candidate_info = {
                    'iteration': iteration,
                    'path':  current_label.path,
                    'path_coords': path_coords,  # âœ… æ·»åŠ åœ°å›¾åæ ‡
                    'distribution': {
                        'values': current_label.distribution.values.tolist(),
                        'weights': current_label.distribution.weights.tolist(),
                        'L1': current_label.distribution.L1
                    },
                    'earliest_arrival': earliest_arrival,
                    'expected_arrival': expected_arrival,
                    'median_arrival': current_label.distribution.get_median(),
                    'std_arrival':  current_label.std_value,
                    'variance': current_label.variance_value,
                    'travel_time':  earliest_arrival - departure_time,
                    'label': current_label,
                    'alpha': alpha,
                    'rank': None,
                    'is_best': False  # âœ… åˆå§‹åŒ–ä¸º False
                }
                
                destination_candidates.append(candidate_info)
                
                print(f"  ğŸ¯ æ‰¾åˆ°å€™é€‰è·¯å¾„#{len(destination_candidates)} è¿­ä»£#{iteration}, "
                    f"Q_Î±={earliest_arrival/10:.1f}åˆ†, "
                    f"Mean={expected_arrival/10:.1f}åˆ†, "
                    f"è·¯å¾„é•¿åº¦={len(current_label.path)}")
                
                # âœ… æ‰¾åˆ°Kæ¡è·¯å¾„åç»§ç»­æœç´¢ï¼ˆç¡®ä¿æ¢ç´¢å……åˆ†ï¼‰
                if len(destination_candidates) >= K:
                    # ç»§ç»­æœç´¢ï¼Œä½†æœ‰ä¸Šé™
                    if len(destination_candidates) >= K * 2:  # æ‰¾åˆ°2Kæ¡ååœæ­¢
                        print(f"\n  âœ“ å·²æ‰¾åˆ° {len(destination_candidates)} æ¡å€™é€‰è·¯å¾„ï¼Œåœæ­¢æœç´¢\n")
                        break
                
                # ç»§ç»­æœç´¢å…¶ä»–è·¯å¾„
                continue
            
            # æ”¯é…æ€§æ£€æŸ¥ï¼ˆè¾ƒå®½æ¾ï¼Œä¿ç•™å¤šæ ·æ€§ï¼‰
            if self._is_dominated(current_label, node_labels[current_label.node_id], alpha):
                self.stats['labels_dominated'] += 1
                continue
            
            self.stats['labels_extended'] += 1
            
            # æ­£å‘æ‰©å±•
            if current_label.node_id not in self.adj_list:
                continue
            
            for successor in self.adj_list[current_label.node_id]: 
                if successor in current_label.path:
                    continue
                
                # æ­£å‘å·ç§¯
                try:
                    def get_link_dist(u, v, slot):
                        return self._get_link_distribution_at_slot(u, v, slot)
                    
                    get_link_dist.__self__ = self
                    
                    new_dist = current_label.distribution.forward_convolve(
                        get_link_dist_func=get_link_dist,
                        current=current_label.node_id,
                        successor=successor,
                        time_intervals_per_day=self.time_intervals_per_day,
                        L2=self.L2,
                        K=self.K
                    )
                    
                    self.stats['convolutions'] += 1
                    
                except Exception as e:
                    if self.verbose and iteration <= 10:
                        print(f"      âš  å·ç§¯å¤±è´¥: {e}")
                    continue
                
                new_cost = new_dist.get_quantile(alpha)
                new_label = ForwardLabel(
                    successor, 
                    new_dist,
                    current_label.path + [successor],
                    new_cost
                )
                
                self.stats['labels_generated'] += 1
                
                # æ”¯é…æ€§å‰ªæ
                if self._is_dominated(new_label, node_labels[successor], alpha):
                    self.stats['labels_dominated'] += 1
                    continue
                
                # åå‘å‰ªæ
                original_count = len(node_labels[successor])
                node_labels[successor] = [
                    old for old in node_labels[successor]
                    if not new_label.dominates_weak(old, alpha)
                ]
                self.stats['labels_dominated'] += (original_count - len(node_labels[successor]))
                
                node_labels[successor].append(new_label)
                node_labels[successor] = self._prune_labels(node_labels[successor], alpha)
                heapq.heappush(open_labels, new_label)
            
            # è¿›åº¦æ˜¾ç¤º
            if not self.verbose and iteration % 100 == 0:
                print(f"  è¿›åº¦: è¿­ä»£#{iteration}, ç”Ÿæˆ{self.stats['labels_generated']: ,}, "
                    f"å€™é€‰{len(destination_candidates)}, "
                    f"å‰ªæ{self.stats['labels_dominated']: ,}", end='\r')
        
        total_time = time.time() - start_time
        
        print(f"\n\n{'='*70}")
        print(f"æœç´¢å®Œæˆ")
        print(f"{'='*70}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤2ï¼šå¯¹Kæ¡å€™é€‰è·¯å¾„æ’åº
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        if not destination_candidates:
            print(f"âœ— æœªæ‰¾åˆ°åˆ°è¾¾ç»ˆç‚¹çš„è·¯å¾„")
            return {
                'success': False,
                'total_time': total_time,
                'iterations': iteration,
                'stats': dict(self.stats),
                'num_candidates': 0
            }
        
        print(f"\næ‰¾åˆ° {len(destination_candidates)} æ¡å€™é€‰è·¯å¾„")
        print(f"å¼€å§‹æ’åºå’Œæ¯”è¾ƒ...\n")
        
        # âœ… å¤šç›®æ ‡æ’åºï¼šä¸»è¦Q_Î±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰ï¼Œæ¬¡è¦Meanï¼Œå†æ¬¡è¦Var
        def rank_score(candidate):
            return (
                candidate['earliest_arrival'],      # ä¸»ç›®æ ‡ï¼šQ_Î±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                candidate['expected_arrival'],      # æ¬¡è¦ï¼šå‡å€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                candidate['variance']               # å†æ¬¡è¦ï¼šæ–¹å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            )
        
        # æ’åºï¼šä»æœ€ä¼˜åˆ°æœ€å·®
        sorted_candidates = sorted(destination_candidates, key=rank_score)
        
        # âœ… è®¾ç½®æ’åå’Œæœ€ä¼˜æ ‡è®°
        for rank, candidate in enumerate(sorted_candidates, 1):
            candidate['rank'] = rank
            candidate['is_best'] = (rank == 1)  # æ’åç¬¬1çš„æ ‡è®°ä¸ºæœ€ä¼˜
        
        # å–å‰Kæ¡
        top_k_candidates = sorted_candidates[:K]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ­¥éª¤3ï¼šè¾“å‡ºç»“æœ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        best_candidate = top_k_candidates[0]
        
        print(f"{'='*70}")
        print(f"Top-{len(top_k_candidates)} å€™é€‰è·¯å¾„å¯¹æ¯”")
        print(f"{'='*70}\n")
        
        print(f"{'æ’å':<6} {'Q_Î±(åˆ†)':<15} {'Mean(åˆ†)':<15} {'Std(åˆ†)':<12} {'æ—…è¡Œæ—¶é—´(åˆ†)':<15} {'è·¯å¾„é•¿åº¦': <10}")
        print(f"{'-'*70}")
        
        for candidate in top_k_candidates: 
            print(f"{candidate['rank']:<6} "
                f"{candidate['earliest_arrival']/10:<15.1f} "
                f"{candidate['expected_arrival']/10:<15.1f} "
                f"{candidate['std_arrival']/10:<12.2f} "
                f"{candidate['travel_time']/10:<15.1f} "
                f"{len(candidate['path']):<10}")
        
        print(f"\n{'='*70}")
        print(f"âœ“ æœ€ä¼˜è·¯å¾„ï¼ˆæ’å#1ï¼‰")
        print(f"{'='*70}")
        print(f"\n  è·¯å¾„:  {self._format_path(best_candidate['path'])}")
        print(f"  é•¿åº¦: {len(best_candidate['path'])} ä¸ªèŠ‚ç‚¹")
        print(f"\n  æ—¶é—´:")
        print(f"    å‡ºå‘æ—¶é—´: {self._time_to_string(departure_time)}")
        print(f"    æœ€æ—©åˆ°è¾¾ (Î±={alpha}): {self._time_to_string(best_candidate['earliest_arrival'])}")
        print(f"    æœŸæœ›åˆ°è¾¾:  {self._time_to_string(best_candidate['expected_arrival'])}")
        print(f"    æ—…è¡Œæ—¶é—´: {best_candidate['travel_time']/10:.1f}åˆ†")
        print(f"    æ ‡å‡†å·®: {best_candidate['std_arrival']/10:.2f}åˆ†")
        print(f"\n  æ€§èƒ½:")
        print(f"    æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"    è¿­ä»£æ¬¡æ•°: {iteration}")
        print(f"    å€™é€‰è·¯å¾„æ•°: {len(destination_candidates)}")
        print(f"    ç”Ÿæˆæ ‡ç­¾:  {self.stats['labels_generated']: ,}")
        print(f"    å‰ªæç‡: {self.stats['labels_dominated']/self.stats['labels_generated']*100:.1f}%")
        print(f"{'='*70}\n")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # æ„å»ºè¿”å›ç»“æœ
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        result = {
            'success': True,
            # æœ€ä¼˜è·¯å¾„ä¿¡æ¯
            'path': best_candidate['path'],
            'path_coords': best_candidate['path_coords'],  # âœ… æ·»åŠ åæ ‡
            'earliest_arrival_time': best_candidate['earliest_arrival'],
            'expected_arrival_time': best_candidate['expected_arrival'],
            'median_arrival_time': best_candidate['median_arrival'],
            'std_arrival_time': best_candidate['std_arrival'],
            'travel_time': best_candidate['travel_time'],
            'distribution': best_candidate['distribution'],  # âœ… å·²ç»æ˜¯å­—å…¸æ ¼å¼
            'departure_time': departure_time,
            
            # Top-Kå€™é€‰è·¯å¾„
            'top_k_candidates':  top_k_candidates,
            'num_candidates': len(destination_candidates),
            'all_candidates': sorted_candidates,
            
            # å…ƒä¿¡æ¯
            'total_time':  total_time,
            'iterations': iteration,
            'alpha': alpha,
            'K': K,
            'origin': origin,
            'destination':  destination,
            'stats': dict(self.stats)
        }
        
        return result


    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # è¾…åŠ©æ–¹æ³•ï¼šè·å–è·¯å¾„åæ ‡
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    def _get_path_coordinates(self, path):
        """
        è·å–è·¯å¾„çš„åœ°ç†åæ ‡
        
        Args:
            path: èŠ‚ç‚¹åˆ—è¡¨
            
        Returns: 
            åæ ‡åˆ—è¡¨ [[lat1, lon1], [lat2, lon2], ...]
        """
        coords = []
        for node in path:
            if node in self.G.nodes:
                node_data = self.G.nodes[node]
                if 'y' in node_data and 'x' in node_data:
                    # Leaflet ä½¿ç”¨ [lat, lon] æ ¼å¼
                    coords.append([node_data['y'], node_data['x']])
        
        return coords


    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ä¿ç•™åŸæœ‰çš„ solve() æ–¹æ³•ï¼ˆå•è·¯å¾„ç‰ˆæœ¬ï¼‰
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    def solve(self, origin:  int, destination: int, departure_time: int,
            alpha: float, max_labels: int = 100000,
            print_interval: int = 100) -> Dict: 
        """
        æ­£å‘æ±‚è§£ï¼šç»™å®šå‡ºå‘æ—¶é—´ï¼Œæ±‚è§£åˆ°è¾¾æ—¶é—´åˆ†å¸ƒå’Œè·¯å¾„ï¼ˆå•è·¯å¾„ç‰ˆæœ¬ï¼‰
        
        å¦‚æœéœ€è¦Kæ¡å€™é€‰è·¯å¾„ï¼Œè¯·ä½¿ç”¨ solve_k_paths() æ–¹æ³•
        
        Args:
            origin: èµ·ç‚¹
            destination: ç»ˆç‚¹
            departure_time: å‡ºå‘æ—¶é—´ï¼ˆ0.1åˆ†é’Ÿå•ä½ï¼‰
            alpha: å¯é æ€§å‚æ•°
            max_labels: æœ€å¤§æ ‡ç­¾æ•°
            print_interval: æ‰“å°é—´éš”
        
        Returns:
            åŒ…å«åˆ°è¾¾æ—¶é—´åˆ†å¸ƒå’Œè·¯å¾„çš„ç»“æœå­—å…¸
        """
        
        # è°ƒç”¨ K-Paths ç‰ˆæœ¬ï¼ŒK=1ï¼ˆåªæ‰¾ä¸€æ¡æœ€ä¼˜è·¯å¾„ï¼‰
        result = self.solve_k_paths(
            origin=origin,
            destination=destination,
            departure_time=departure_time,
            alpha=alpha,
            K=1,  # åªæ‰¾1æ¡è·¯å¾„
            max_labels=max_labels,
            print_interval=print_interval
        )
        
        # ç®€åŒ–è¿”å›ç»“æœï¼ˆç§»é™¤K-Pathsç›¸å…³å­—æ®µï¼‰
        if result['success']:
            result.pop('top_k_candidates', None)
            result.pop('all_candidates', None)
            result.pop('num_candidates', None)
            result.pop('K', None)
        
        return result

    def _is_dominated(self, label: ForwardLabel, existing_labels: List[ForwardLabel],
                     alpha: float) -> bool:
        """æ”¯é…æ€§æ£€æŸ¥"""
        if len(existing_labels) < self.max_labels_per_node:
            domination_count = 0
            for existing in existing_labels:
                if existing.dominates_weak(label, alpha):
                    domination_count += 1
            return domination_count >= 2
        
        for existing in existing_labels:
            if existing.dominates_weak(label, alpha):
                return True
        
        return False
    
    def _prune_labels(self, labels: List[ForwardLabel], alpha: float) -> List[ForwardLabel]:
        """æ ‡ç­¾å‰ªæ"""
        if len(labels) <= self.max_labels_per_node:
            return labels
        
        def label_score(label):
            q = label.distribution.get_quantile(alpha)
            return (q, label.mean_cache, label.variance_cache)
        
        sorted_labels = sorted(labels, key=label_score)
        return sorted_labels[:self.max_labels_per_node]
    
    def _format_path(self, path: List[int]) -> str:
        if len(path) <= 10:
            return ' â†’ '.join(map(str, path))
        return f"{' â†’ '.join(map(str, path[: 5]))} â†’ ...â†’ {' â†’ '.join(map(str, path[-3:]))}"
    
    def _time_to_string(self, time_01min):
        """æ—¶é—´æ ¼å¼è½¬æ¢"""
        total_minutes = time_01min / 10
        hours = int(total_minutes // 60)
        minutes = int(total_minutes % 60)
        return f"{hours:02d}:{minutes:02d}"